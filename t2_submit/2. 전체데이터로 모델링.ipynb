{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2464d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# ğŸ§¹ \"ìµœì¢… ë°ì´í„° ì •ì œ ìŠ¤í¬ë¦½íŠ¸\" (ì´ê²ƒë§Œ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”)\n",
    "# ----------------------------------------------------\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ§¹ ë°ì´í„° ì •ì œ: Flow-Packet ë™ê¸°í™” ì¤‘ë³µ ì œê±°\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. ì›ë³¸ ë°ì´í„° ë¡œë”©\n",
    "print(\"ğŸ“‚ 1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ë¡œë”©...\")\n",
    "flow_data = joblib.load(\"task2_data/train_flow_data.pkl\")\n",
    "\n",
    "all_packets = []\n",
    "packet_files = [f\"task2_data/train_packet_data_{i}.pkl\" for i in range(50000, 650000, 50000)]\n",
    "for file_path in packet_files:\n",
    "    if os.path.exists(file_path):\n",
    "        packets_chunk = joblib.load(file_path)\n",
    "        all_packets.extend(packets_chunk)\n",
    "print(f\"âœ… Flow/Packet ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "# 2. 'Flow' ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µë˜ì§€ ì•Šì€ 'ì›ë³¸ ì¸ë±ìŠ¤' í™•ë³´\n",
    "print(\"\\nğŸ” 2ë‹¨ê³„: ìœ íš¨ ì¸ë±ìŠ¤ ì‹ë³„...\")\n",
    "initial_rows = len(flow_data)\n",
    "non_duplicate_indices = flow_data.drop_duplicates().index\n",
    "final_rows = len(non_duplicate_indices)\n",
    "print(f\"âœ… ìœ íš¨ ì¸ë±ìŠ¤ {final_rows:,}ê°œ í™•ë³´ (ì œê±°ë  ì¤‘ë³µ: {initial_rows - final_rows:,}ê°œ)\")\n",
    "\n",
    "# 3. 'ìœ íš¨ ì¸ë±ìŠ¤'ë¥¼ ì‚¬ìš©í•˜ì—¬ Flowì™€ Packet ë°ì´í„° ë™ì‹œ ì •ì œ\n",
    "print(\"\\nğŸ”„ 3ë‹¨ê³„: ë°ì´í„° ë™ê¸°í™” ì •ì œ...\")\n",
    "# ğŸ”¥ ì˜¤ë¥˜ ìˆ˜ì •: reset_index(drop=True)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•Šì•„, ì›ë³¸ê³¼ì˜ ì—°ê²°ì„ ìœ ì§€í•´ì•¼ í•˜ì§€ë§Œ,\n",
    "# ìµœì¢… ì €ì¥ íŒŒì¼ì€ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ê°–ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” reset_indexë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì¤‘ìš”í•œ ê²ƒì€ Packet ë°ì´í„°ë„ ë™ì¼í•œ ìˆœì„œë¡œ ì •ë ¬ëœ í›„ì— ì €ì¥ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
    "flow_data_cleaned = flow_data.loc[non_duplicate_indices].reset_index(drop=True)\n",
    "packet_data_cleaned = [all_packets[i] for i in non_duplicate_indices]\n",
    "print(f\"âœ… ë™ê¸°í™” ì™„ë£Œ! ìµœì¢… ë°ì´í„°: {len(flow_data_cleaned):,}ê°œ\")\n",
    "\n",
    "# 4. ì •ì œëœ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n",
    "print(\"\\nğŸ’¾ 4ë‹¨ê³„: ì •ì œëœ ë°ì´í„°ì…‹ ì €ì¥...\")\n",
    "cleaned_flow_path = \"task2_data/train_flow_data_cleaned.pkl\"\n",
    "cleaned_packet_path = \"task2_data/train_packet_data_cleaned.pkl\"\n",
    "joblib.dump(flow_data_cleaned, cleaned_flow_path)\n",
    "joblib.dump(packet_data_cleaned, cleaned_packet_path)\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {cleaned_flow_path}, {cleaned_packet_path}\")\n",
    "print(f\"\\nğŸ‰ ëª¨ë“  ë°ì´í„° ì •ì œ ì‘ì—… ì™„ë£Œ!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸŒ ì „ì²´ ë°ì´í„° ê· ë“± ì¸µí™” ìƒ˜í”Œë§ + 4ê°œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° íŒŒì¼ ìŠ¤ìº”\n",
      "==================================================\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_50000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_100000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_150000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_200000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_250000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_300000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_350000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_400000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_450000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_500000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_550000.pkl\n",
      "âœ“ ë°œê²¬: task2_data/train_packet_data_600000.pkl\n",
      "\n",
      "ğŸ“Š ì´ 12ê°œ íŒ¨í‚· íŒŒì¼ ë°œê²¬\n",
      "\n",
      "ğŸ“ˆ Flow ë°ì´í„° ë¡œë”© ë° í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„...\n",
      "âœ“ Flow ë°ì´í„° í¬ê¸°: (600000, 11)\n",
      "\n",
      "ğŸ“Š ì „ì²´ Duration Class ë¶„í¬:\n",
      "   Class 0: 23,407ê°œ (3.9%)\n",
      "   Class 1: 212,796ê°œ (35.5%)\n",
      "   Class 2: 140,943ê°œ (23.5%)\n",
      "   Class 3: 222,854ê°œ (37.1%)\n",
      "\n",
      "ğŸ“Š ì „ì²´ Volume Class ë¶„í¬:\n",
      "   Class 0: 205,670ê°œ (34.3%)\n",
      "   Class 1: 145,515ê°œ (24.3%)\n",
      "   Class 2: 213,478ê°œ (35.6%)\n",
      "   Class 3: 35,337ê°œ (5.9%)\n",
      "\n",
      "ğŸ¯ 2ë‹¨ê³„: ê· ë“± + ì¸µí™” ìƒ˜í”Œë§ ì „ëµ\n",
      "==================================================\n",
      "ğŸ¯ ëª©í‘œ ì´ ìƒ˜í”Œ ìˆ˜: 50,000ê°œ\n",
      "ğŸ“ íŒŒì¼ë‹¹ ê· ë“± ìƒ˜í”Œ ìˆ˜: 4,166ê°œ\n",
      "ğŸ“Š ì¸µí™” ì¶”ì¶œ ê¸°ì¤€: Duration + Volume ë³µí•© í´ë˜ìŠ¤ ë¹„ìœ¨ ë³´ì¡´\n",
      "\n",
      "âš–ï¸ 3ë‹¨ê³„: ê° íŒŒì¼ì—ì„œ ê· ë“± + ì¸µí™” ìƒ˜í”Œë§ ì‹¤í–‰\n",
      "==================================================\n",
      "\n",
      "ğŸ“ íŒŒì¼ 1/12: task2_data/train_packet_data_50000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 0 ~ 49,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(2572), 1: np.int64(20191), 2: np.int64(11916), 3: np.int64(15321)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(18372), 1: np.int64(14692), 2: np.int64(15202), 3: np.int64(1734)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(215), 1: np.int64(1682), 2: np.int64(992), 3: np.int64(1277)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1531), 1: np.int64(1224), 2: np.int64(1267), 3: np.int64(144)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.051), 1: np.float64(0.404), 2: np.float64(0.238), 3: np.float64(0.306)} vs ìƒ˜í”Œ{0: np.float64(0.052), 1: np.float64(0.404), 2: np.float64(0.238), 3: np.float64(0.307)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.367), 1: np.float64(0.294), 2: np.float64(0.304), 3: np.float64(0.035)} vs ìƒ˜í”Œ{0: np.float64(0.367), 1: np.float64(0.294), 2: np.float64(0.304), 3: np.float64(0.035)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 2/12: task2_data/train_packet_data_100000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 50,000 ~ 99,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1876), 1: np.int64(16417), 2: np.int64(14377), 3: np.int64(17330)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(15331), 1: np.int64(12220), 2: np.int64(19157), 3: np.int64(3292)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(156), 1: np.int64(1368), 2: np.int64(1197), 3: np.int64(1445)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1278), 1: np.int64(1018), 2: np.int64(1596), 3: np.int64(274)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.038), 1: np.float64(0.328), 2: np.float64(0.288), 3: np.float64(0.347)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.328), 2: np.float64(0.287), 3: np.float64(0.347)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)} vs ìƒ˜í”Œ{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 3/12: task2_data/train_packet_data_150000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 100,000 ~ 149,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1808), 1: np.int64(18190), 2: np.int64(11240), 3: np.int64(18762)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16568), 1: np.int64(11811), 2: np.int64(18245), 3: np.int64(3376)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(151), 1: np.int64(1515), 2: np.int64(936), 3: np.int64(1564)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1381), 1: np.int64(984), 2: np.int64(1520), 3: np.int64(281)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)} vs ìƒ˜í”Œ{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.068)} vs ìƒ˜í”Œ{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.067)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 4/12: task2_data/train_packet_data_200000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 150,000 ~ 199,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1787), 1: np.int64(17473), 2: np.int64(11322), 3: np.int64(19418)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16828), 1: np.int64(11541), 2: np.int64(18425), 3: np.int64(3206)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 16ê°œ ì¡°í•©\n",
      "   âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "ğŸ“ íŒŒì¼ 5/12: task2_data/train_packet_data_250000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 200,000 ~ 249,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1829), 1: np.int64(17848), 2: np.int64(11066), 3: np.int64(19257)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(19092), 1: np.int64(11493), 2: np.int64(16530), 3: np.int64(2885)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(156), 1: np.int64(1368), 2: np.int64(1197), 3: np.int64(1445)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1278), 1: np.int64(1018), 2: np.int64(1596), 3: np.int64(274)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.038), 1: np.float64(0.328), 2: np.float64(0.288), 3: np.float64(0.347)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.328), 2: np.float64(0.287), 3: np.float64(0.347)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)} vs ìƒ˜í”Œ{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 3/12: task2_data/train_packet_data_150000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 100,000 ~ 149,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1808), 1: np.int64(18190), 2: np.int64(11240), 3: np.int64(18762)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16568), 1: np.int64(11811), 2: np.int64(18245), 3: np.int64(3376)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(151), 1: np.int64(1515), 2: np.int64(936), 3: np.int64(1564)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1381), 1: np.int64(984), 2: np.int64(1520), 3: np.int64(281)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)} vs ìƒ˜í”Œ{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.068)} vs ìƒ˜í”Œ{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.067)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 4/12: task2_data/train_packet_data_200000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 150,000 ~ 199,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1787), 1: np.int64(17473), 2: np.int64(11322), 3: np.int64(19418)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16828), 1: np.int64(11541), 2: np.int64(18425), 3: np.int64(3206)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 16ê°œ ì¡°í•©\n",
      "   âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "ğŸ“ íŒŒì¼ 5/12: task2_data/train_packet_data_250000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 200,000 ~ 249,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1829), 1: np.int64(17848), 2: np.int64(11066), 3: np.int64(19257)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(19092), 1: np.int64(11493), 2: np.int64(16530), 3: np.int64(2885)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(153), 1: np.int64(1487), 2: np.int64(922), 3: np.int64(1604)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1591), 1: np.int64(957), 2: np.int64(1378), 3: np.int64(240)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)} vs ìƒ˜í”Œ{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 6/12: task2_data/train_packet_data_300000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 250,000 ~ 299,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1858), 1: np.int64(17656), 2: np.int64(11544), 3: np.int64(18942)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(17991), 1: np.int64(11939), 2: np.int64(17284), 3: np.int64(2786)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(155), 1: np.int64(1471), 2: np.int64(962), 3: np.int64(1578)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1499), 1: np.int64(994), 2: np.int64(1440), 3: np.int64(233)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)} vs ìƒ˜í”Œ{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 7/12: task2_data/train_packet_data_350000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 300,000 ~ 349,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1811), 1: np.int64(17557), 2: np.int64(10985), 3: np.int64(19647)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16768), 1: np.int64(11675), 2: np.int64(18284), 3: np.int64(3273)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(151), 1: np.int64(1463), 2: np.int64(915), 3: np.int64(1637)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1398), 1: np.int64(973), 2: np.int64(1523), 3: np.int64(272)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)} vs ìƒ˜í”Œ{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.335), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)} vs ìƒ˜í”Œ{0: np.float64(0.336), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 8/12: task2_data/train_packet_data_400000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 350,000 ~ 399,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1723), 1: np.int64(17448), 2: np.int64(11855), 3: np.int64(18974)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16599), 1: np.int64(12429), 2: np.int64(17997), 3: np.int64(2975)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(144), 1: np.int64(1454), 2: np.int64(987), 3: np.int64(1581)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1382), 1: np.int64(1036), 2: np.int64(1500), 3: np.int64(248)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.034), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.379)} vs ìƒ˜í”Œ{0: np.float64(0.035), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.38)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)} vs ìƒ˜í”Œ{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 9/12: task2_data/train_packet_data_450000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 400,000 ~ 449,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1916), 1: np.int64(17477), 2: np.int64(11474), 3: np.int64(19133)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16755), 1: np.int64(11853), 2: np.int64(18227), 3: np.int64(3165)}\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(153), 1: np.int64(1487), 2: np.int64(922), 3: np.int64(1604)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1591), 1: np.int64(957), 2: np.int64(1378), 3: np.int64(240)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)} vs ìƒ˜í”Œ{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 6/12: task2_data/train_packet_data_300000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 250,000 ~ 299,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1858), 1: np.int64(17656), 2: np.int64(11544), 3: np.int64(18942)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(17991), 1: np.int64(11939), 2: np.int64(17284), 3: np.int64(2786)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(155), 1: np.int64(1471), 2: np.int64(962), 3: np.int64(1578)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1499), 1: np.int64(994), 2: np.int64(1440), 3: np.int64(233)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)} vs ìƒ˜í”Œ{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)} vs ìƒ˜í”Œ{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 7/12: task2_data/train_packet_data_350000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 300,000 ~ 349,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1811), 1: np.int64(17557), 2: np.int64(10985), 3: np.int64(19647)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16768), 1: np.int64(11675), 2: np.int64(18284), 3: np.int64(3273)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(151), 1: np.int64(1463), 2: np.int64(915), 3: np.int64(1637)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1398), 1: np.int64(973), 2: np.int64(1523), 3: np.int64(272)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)} vs ìƒ˜í”Œ{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.335), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)} vs ìƒ˜í”Œ{0: np.float64(0.336), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 8/12: task2_data/train_packet_data_400000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 350,000 ~ 399,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1723), 1: np.int64(17448), 2: np.int64(11855), 3: np.int64(18974)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16599), 1: np.int64(12429), 2: np.int64(17997), 3: np.int64(2975)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(144), 1: np.int64(1454), 2: np.int64(987), 3: np.int64(1581)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1382), 1: np.int64(1036), 2: np.int64(1500), 3: np.int64(248)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.034), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.379)} vs ìƒ˜í”Œ{0: np.float64(0.035), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.38)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)} vs ìƒ˜í”Œ{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 9/12: task2_data/train_packet_data_450000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 400,000 ~ 449,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1916), 1: np.int64(17477), 2: np.int64(11474), 3: np.int64(19133)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16755), 1: np.int64(11853), 2: np.int64(18227), 3: np.int64(3165)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(160), 1: np.int64(1456), 2: np.int64(955), 3: np.int64(1595)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1396), 1: np.int64(988), 2: np.int64(1518), 3: np.int64(264)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.038), 1: np.float64(0.35), 2: np.float64(0.229), 3: np.float64(0.383)} vs ìƒ˜í”Œ{0: np.float64(0.038), 1: np.float64(0.349), 2: np.float64(0.229), 3: np.float64(0.383)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.365), 3: np.float64(0.063)} vs ìƒ˜í”Œ{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.364), 3: np.float64(0.063)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 10/12: task2_data/train_packet_data_500000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 450,000 ~ 499,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1764), 1: np.int64(16808), 2: np.int64(12126), 3: np.int64(19302)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16306), 1: np.int64(11789), 2: np.int64(18675), 3: np.int64(3230)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(147), 1: np.int64(1400), 2: np.int64(1010), 3: np.int64(1609)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1358), 1: np.int64(982), 2: np.int64(1557), 3: np.int64(269)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.243), 3: np.float64(0.386)} vs ìƒ˜í”Œ{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.242), 3: np.float64(0.386)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)} vs ìƒ˜í”Œ{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 11/12: task2_data/train_packet_data_550000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 500,000 ~ 549,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1954), 1: np.int64(17090), 2: np.int64(11755), 3: np.int64(19201)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16045), 1: np.int64(11920), 2: np.int64(18991), 3: np.int64(3044)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 16ê°œ ì¡°í•©\n",
      "   âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "ğŸ“ íŒŒì¼ 12/12: task2_data/train_packet_data_600000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 550,000 ~ 599,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(2509), 1: np.int64(18641), 2: np.int64(11283), 3: np.int64(17567)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(19015), 1: np.int64(12153), 2: np.int64(16461), 3: np.int64(2371)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(160), 1: np.int64(1456), 2: np.int64(955), 3: np.int64(1595)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1396), 1: np.int64(988), 2: np.int64(1518), 3: np.int64(264)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.038), 1: np.float64(0.35), 2: np.float64(0.229), 3: np.float64(0.383)} vs ìƒ˜í”Œ{0: np.float64(0.038), 1: np.float64(0.349), 2: np.float64(0.229), 3: np.float64(0.383)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.365), 3: np.float64(0.063)} vs ìƒ˜í”Œ{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.364), 3: np.float64(0.063)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 10/12: task2_data/train_packet_data_500000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 450,000 ~ 499,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1764), 1: np.int64(16808), 2: np.int64(12126), 3: np.int64(19302)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16306), 1: np.int64(11789), 2: np.int64(18675), 3: np.int64(3230)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(147), 1: np.int64(1400), 2: np.int64(1010), 3: np.int64(1609)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1358), 1: np.int64(982), 2: np.int64(1557), 3: np.int64(269)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.243), 3: np.float64(0.386)} vs ìƒ˜í”Œ{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.242), 3: np.float64(0.386)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)} vs ìƒ˜í”Œ{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)}\n",
      "\n",
      "ğŸ“ íŒŒì¼ 11/12: task2_data/train_packet_data_550000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 500,000 ~ 549,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(1954), 1: np.int64(17090), 2: np.int64(11755), 3: np.int64(19201)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(16045), 1: np.int64(11920), 2: np.int64(18991), 3: np.int64(3044)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 16ê°œ ì¡°í•©\n",
      "   âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "ğŸ“ íŒŒì¼ 12/12: task2_data/train_packet_data_600000.pkl\n",
      "   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: 550,000 ~ 599,999 (50,000ê°œ)\n",
      "   ğŸ¯ Duration ë¶„í¬: {0: np.int64(2509), 1: np.int64(18641), 2: np.int64(11283), 3: np.int64(17567)}\n",
      "   ğŸ“¦ Volume ë¶„í¬: {0: np.int64(19015), 1: np.int64(12153), 2: np.int64(16461), 3: np.int64(2371)}\n",
      "   ğŸ”— ë³µí•©ì¸µ(D_V): 15ê°œ ì¡°í•©\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(209), 1: np.int64(1553), 2: np.int64(940), 3: np.int64(1464)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1584), 1: np.int64(1013), 2: np.int64(1371), 3: np.int64(198)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)} vs ìƒ˜í”Œ{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.047)} vs ìƒ˜í”Œ{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.048)}\n",
      "\n",
      "âœ… ì „ì²´ ìƒ˜í”Œë§ ì™„ë£Œ!\n",
      "ğŸ“Š ì´ ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤: 41,660ê°œ\n",
      "ğŸ¯ ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ : 83.3%\n",
      "\n",
      "ğŸ“Š 4ë‹¨ê³„: ìµœì¢… ìƒ˜í”Œë§ ê²°ê³¼ ê²€ì¦\n",
      "==================================================\n",
      "ğŸ¯ ìµœì¢… ìƒ˜í”Œë§ëœ Duration Class ë¶„í¬:\n",
      "   Class 0: 1,641ê°œ (3.9%) [ì›ë³¸: 3.9%]\n",
      "   Class 1: 14,849ê°œ (35.6%) [ì›ë³¸: 35.5%]\n",
      "   Class 2: 9,816ê°œ (23.6%) [ì›ë³¸: 23.5%]\n",
      "   Class 3: 15,354ê°œ (36.9%) [ì›ë³¸: 37.1%]\n",
      "\n",
      "ğŸ“¦ ìµœì¢… ìƒ˜í”Œë§ëœ Volume Class ë¶„í¬:\n",
      "   Class 0: 14,398ê°œ (34.6%) [ì›ë³¸: 34.3%]\n",
      "   Class 1: 10,169ê°œ (24.4%) [ì›ë³¸: 24.3%]\n",
      "   Class 2: 14,670ê°œ (35.2%) [ì›ë³¸: 35.6%]\n",
      "   Class 3: 2,423ê°œ (5.8%) [ì›ë³¸: 5.9%]\n",
      "\n",
      "âš–ï¸ ë³µí•© ì¸µí™” ë¹„ìœ¨ ë³´ì¡´ë„:\n",
      "   Duration: 99.4%\n",
      "   Volume: 99.1%\n",
      "   ì „ì²´ í‰ê· : 99.2% (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½)\n",
      "\n",
      "âœ… ë³µí•© ì¸µí™” ìƒ˜í”Œë§ ì™„ë£Œ! Duration+Volume ë™ì‹œ ë¹„ìœ¨ ë³´ì¡´ìœ¼ë¡œ ìµœê³  í’ˆì§ˆ ìƒ˜í”Œ í™•ë³´! ğŸ‰\n",
      "   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: 4,166ê°œ\n",
      "   ğŸ“ˆ ìƒ˜í”Œ Duration: {0: np.int64(209), 1: np.int64(1553), 2: np.int64(940), 3: np.int64(1464)}\n",
      "   ğŸ“¦ ìƒ˜í”Œ Volume: {0: np.int64(1584), 1: np.int64(1013), 2: np.int64(1371), 3: np.int64(198)}\n",
      "   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)} vs ìƒ˜í”Œ{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)}\n",
      "   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.047)} vs ìƒ˜í”Œ{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.048)}\n",
      "\n",
      "âœ… ì „ì²´ ìƒ˜í”Œë§ ì™„ë£Œ!\n",
      "ğŸ“Š ì´ ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤: 41,660ê°œ\n",
      "ğŸ¯ ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ : 83.3%\n",
      "\n",
      "ğŸ“Š 4ë‹¨ê³„: ìµœì¢… ìƒ˜í”Œë§ ê²°ê³¼ ê²€ì¦\n",
      "==================================================\n",
      "ğŸ¯ ìµœì¢… ìƒ˜í”Œë§ëœ Duration Class ë¶„í¬:\n",
      "   Class 0: 1,641ê°œ (3.9%) [ì›ë³¸: 3.9%]\n",
      "   Class 1: 14,849ê°œ (35.6%) [ì›ë³¸: 35.5%]\n",
      "   Class 2: 9,816ê°œ (23.6%) [ì›ë³¸: 23.5%]\n",
      "   Class 3: 15,354ê°œ (36.9%) [ì›ë³¸: 37.1%]\n",
      "\n",
      "ğŸ“¦ ìµœì¢… ìƒ˜í”Œë§ëœ Volume Class ë¶„í¬:\n",
      "   Class 0: 14,398ê°œ (34.6%) [ì›ë³¸: 34.3%]\n",
      "   Class 1: 10,169ê°œ (24.4%) [ì›ë³¸: 24.3%]\n",
      "   Class 2: 14,670ê°œ (35.2%) [ì›ë³¸: 35.6%]\n",
      "   Class 3: 2,423ê°œ (5.8%) [ì›ë³¸: 5.9%]\n",
      "\n",
      "âš–ï¸ ë³µí•© ì¸µí™” ë¹„ìœ¨ ë³´ì¡´ë„:\n",
      "   Duration: 99.4%\n",
      "   Volume: 99.1%\n",
      "   ì „ì²´ í‰ê· : 99.2% (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½)\n",
      "\n",
      "âœ… ë³µí•© ì¸µí™” ìƒ˜í”Œë§ ì™„ë£Œ! Duration+Volume ë™ì‹œ ë¹„ìœ¨ ë³´ì¡´ìœ¼ë¡œ ìµœê³  í’ˆì§ˆ ìƒ˜í”Œ í™•ë³´! ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ ì „ì²´ ë°ì´í„° ê· ë“± ì¸µí™” ìƒ˜í”Œë§ ë° ëª¨ë¸ë§\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸŒ ì „ì²´ ë°ì´í„° ê· ë“± ì¸µí™” ìƒ˜í”Œë§ + 4ê°œ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 1ë‹¨ê³„: ì „ì²´ íŒŒì¼ ëª©ë¡ ë° ì •ë³´ í™•ì¸\n",
    "print(\"\\nğŸ“‚ 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° íŒŒì¼ ìŠ¤ìº”\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# íŒ¨í‚· ë°ì´í„° íŒŒì¼ë“¤ (50000ë¶€í„° 600000ê¹Œì§€ 50000ì”©)\n",
    "packet_files = []\n",
    "for i in range(50000, 650000, 50000):  # 50000, 100000, ..., 600000\n",
    "    file_path = f\"task2_data/train_packet_data_{i}.pkl\"\n",
    "    if os.path.exists(file_path):\n",
    "        packet_files.append(file_path)\n",
    "        print(f\"âœ“ ë°œê²¬: {file_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ ì—†ìŒ: {file_path}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ {len(packet_files)}ê°œ íŒ¨í‚· íŒŒì¼ ë°œê²¬\")\n",
    "\n",
    "# Flow ë°ì´í„° ë¡œë”© (í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ìš©)\n",
    "print(f\"\\nğŸ“ˆ Flow ë°ì´í„° ë¡œë”© ë° í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„...\")\n",
    "flow_data = joblib.load(\"task2_data/train_flow_data.pkl\")\n",
    "print(f\"âœ“ Flow ë°ì´í„° í¬ê¸°: {flow_data.shape}\")\n",
    "\n",
    "# ì „ì²´ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "duration_dist = flow_data['duration_class'].value_counts().sort_index()\n",
    "volume_dist = flow_data['volume_class'].value_counts().sort_index()\n",
    "\n",
    "print(f\"\\nğŸ“Š ì „ì²´ Duration Class ë¶„í¬:\")\n",
    "for cls, count in duration_dist.items():\n",
    "    percentage = (count / len(flow_data)) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì „ì²´ Volume Class ë¶„í¬:\")\n",
    "for cls, count in volume_dist.items():\n",
    "    percentage = (count / len(flow_data)) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# 2ë‹¨ê³„: ê· ë“± + ì¸µí™” ìƒ˜í”Œë§ ì „ëµ\n",
    "print(f\"\\nğŸ¯ 2ë‹¨ê³„: ê· ë“± + ì¸µí™” ìƒ˜í”Œë§ ì „ëµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ëª©í‘œ ìƒ˜í”Œ ìˆ˜ ì„¤ì •\n",
    "target_total_samples = 50000  # ì „ì²´ ëª©í‘œ ìƒ˜í”Œ ìˆ˜\n",
    "samples_per_file = target_total_samples // len(packet_files)  # íŒŒì¼ë‹¹ ê· ë“± ìƒ˜í”Œ ìˆ˜\n",
    "\n",
    "print(f\"ğŸ¯ ëª©í‘œ ì´ ìƒ˜í”Œ ìˆ˜: {target_total_samples:,}ê°œ\")\n",
    "print(f\"ğŸ“ íŒŒì¼ë‹¹ ê· ë“± ìƒ˜í”Œ ìˆ˜: {samples_per_file:,}ê°œ\")\n",
    "print(f\"ğŸ“Š ì¸µí™” ì¶”ì¶œ ê¸°ì¤€: Duration + Volume ë³µí•© í´ë˜ìŠ¤ ë¹„ìœ¨ ë³´ì¡´\")\n",
    "\n",
    "def stratified_sample_from_file(file_path, sample_size, flow_data_subset):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ ë‚´ì—ì„œ ì¸µí™” ì¶”ì¶œ ìˆ˜í–‰ - Duration + Volume ë³µí•© ì¸µí™”\n",
    "    \"\"\"\n",
    "    # Duration + Volume ë³µí•© í´ë˜ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì¸µí™” ì¶”ì¶œ\n",
    "    stratify_key = flow_data_subset['duration_class'].astype(str) + '_' + flow_data_subset['volume_class'].astype(str)\n",
    "    \n",
    "    stratified_sampler = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        train_size=sample_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    indices = np.arange(len(flow_data_subset))\n",
    "    stratified_indices, _ = next(stratified_sampler.split(indices, stratify_key))\n",
    "    \n",
    "    return stratified_indices\n",
    "\n",
    "# 3ë‹¨ê³„: ê° íŒŒì¼ì—ì„œ ê· ë“± + ì¸µí™” ìƒ˜í”Œë§\n",
    "print(f\"\\nâš–ï¸ 3ë‹¨ê³„: ê° íŒŒì¼ì—ì„œ ê· ë“± + ì¸µí™” ìƒ˜í”Œë§ ì‹¤í–‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sampled_indices = []\n",
    "total_sampled = 0\n",
    "\n",
    "for i, file_path in enumerate(packet_files, 1):\n",
    "    print(f\"\\nğŸ“ íŒŒì¼ {i}/{len(packet_files)}: {file_path}\")\n",
    "    \n",
    "    # íŒŒì¼ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: train_packet_data_100000.pkl -> 100000)\n",
    "    file_num = int(file_path.split('_')[-1].split('.')[0])\n",
    "    \n",
    "    # í•´ë‹¹ íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” flow ë°ì´í„° ë²”ìœ„\n",
    "    start_idx = (i-1) * 50000\n",
    "    end_idx = min(start_idx + 50000, len(flow_data))\n",
    "    flow_subset = flow_data.iloc[start_idx:end_idx].copy()\n",
    "    flow_subset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"   ğŸ“Š ëŒ€ì‘ Flow ë²”ìœ„: {start_idx:,} ~ {end_idx-1:,} ({len(flow_subset):,}ê°œ)\")\n",
    "    \n",
    "    # ì´ êµ¬ê°„ì˜ í´ë˜ìŠ¤ ë¶„í¬\n",
    "    subset_duration_dist = flow_subset['duration_class'].value_counts().sort_index()\n",
    "    subset_volume_dist = flow_subset['volume_class'].value_counts().sort_index()\n",
    "    print(f\"   ğŸ¯ Duration ë¶„í¬: {dict(subset_duration_dist)}\")\n",
    "    print(f\"   ğŸ“¦ Volume ë¶„í¬: {dict(subset_volume_dist)}\")\n",
    "    \n",
    "    # ë³µí•© ì¸µí™” í‚¤ ë¶„í¬ í™•ì¸\n",
    "    subset_stratify_key = flow_subset['duration_class'].astype(str) + '_' + flow_subset['volume_class'].astype(str)\n",
    "    subset_combined_dist = subset_stratify_key.value_counts().sort_index()\n",
    "    print(f\"   ğŸ”— ë³µí•©ì¸µ(D_V): {len(subset_combined_dist)}ê°œ ì¡°í•©\")\n",
    "    \n",
    "    # ì¸µí™” ìƒ˜í”Œë§ ì‹¤í–‰\n",
    "    try:\n",
    "        actual_sample_size = min(samples_per_file, len(flow_subset))\n",
    "        stratified_indices = stratified_sample_from_file(file_path, actual_sample_size, flow_subset)\n",
    "        \n",
    "        # ì „ì²´ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (start_idx ë”í•˜ê¸°)\n",
    "        global_indices = [start_idx + idx for idx in stratified_indices]\n",
    "        sampled_indices.extend(global_indices)\n",
    "        \n",
    "        total_sampled += len(stratified_indices)\n",
    "        \n",
    "        # ìƒ˜í”Œë§ëœ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "        sampled_flow = flow_subset.iloc[stratified_indices]\n",
    "        sampled_duration_dist = sampled_flow['duration_class'].value_counts().sort_index()\n",
    "        sampled_volume_dist = sampled_flow['volume_class'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(stratified_indices):,}ê°œ\")\n",
    "        print(f\"   ğŸ“ˆ ìƒ˜í”Œ Duration: {dict(sampled_duration_dist)}\")\n",
    "        print(f\"   ğŸ“¦ ìƒ˜í”Œ Volume: {dict(sampled_volume_dist)}\")\n",
    "        \n",
    "        # ë³µí•© ë¹„ìœ¨ ë³´ì¡´ í™•ì¸\n",
    "        original_duration_ratios = (subset_duration_dist / len(flow_subset)).round(3)\n",
    "        sampled_duration_ratios = (sampled_duration_dist / len(stratified_indices)).round(3)\n",
    "        original_volume_ratios = (subset_volume_dist / len(flow_subset)).round(3)\n",
    "        sampled_volume_ratios = (sampled_volume_dist / len(stratified_indices)).round(3)\n",
    "        \n",
    "        print(f\"   âš–ï¸ Duration ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{dict(original_duration_ratios)} vs ìƒ˜í”Œ{dict(sampled_duration_ratios)}\")\n",
    "        print(f\"   âš–ï¸ Volume ë¹„ìœ¨ë³´ì¡´: ì›ë³¸{dict(original_volume_ratios)} vs ìƒ˜í”Œ{dict(sampled_volume_ratios)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ìƒ˜í”Œë§ ì‹¤íŒ¨: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ ìƒ˜í”Œë§ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ì´ ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤: {len(sampled_indices):,}ê°œ\")\n",
    "print(f\"ğŸ¯ ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ : {len(sampled_indices)/target_total_samples*100:.1f}%\")\n",
    "\n",
    "# 4ë‹¨ê³„: ìƒ˜í”Œë§ëœ ë°ì´í„°ì˜ ìµœì¢… í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "print(f\"\\nğŸ“Š 4ë‹¨ê³„: ìµœì¢… ìƒ˜í”Œë§ ê²°ê³¼ ê²€ì¦\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ìƒ˜í”Œë§ëœ flow ë°ì´í„°\n",
    "sampled_flow_data = flow_data.iloc[sampled_indices].copy()\n",
    "sampled_flow_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_duration_dist = sampled_flow_data['duration_class'].value_counts().sort_index()\n",
    "final_volume_dist = sampled_flow_data['volume_class'].value_counts().sort_index()\n",
    "\n",
    "print(f\"ğŸ¯ ìµœì¢… ìƒ˜í”Œë§ëœ Duration Class ë¶„í¬:\")\n",
    "for cls, count in final_duration_dist.items():\n",
    "    original_ratio = duration_dist[cls] / len(flow_data) * 100\n",
    "    sampled_ratio = count / len(sampled_flow_data) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}ê°œ ({sampled_ratio:.1f}%) [ì›ë³¸: {original_ratio:.1f}%]\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ ìµœì¢… ìƒ˜í”Œë§ëœ Volume Class ë¶„í¬:\")\n",
    "for cls, count in final_volume_dist.items():\n",
    "    original_ratio = volume_dist[cls] / len(flow_data) * 100\n",
    "    sampled_ratio = count / len(sampled_flow_data) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}ê°œ ({sampled_ratio:.1f}%) [ì›ë³¸: {original_ratio:.1f}%]\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¹„ìœ¨ ë³´ì¡´ë„ ì¸¡ì • (Duration + Volume ëª¨ë‘)\n",
    "duration_preservation = []\n",
    "for cls in final_duration_dist.index:\n",
    "    original_ratio = duration_dist[cls] / len(flow_data)\n",
    "    sampled_ratio = final_duration_dist[cls] / len(sampled_flow_data)\n",
    "    preservation = min(sampled_ratio/original_ratio, original_ratio/sampled_ratio) * 100\n",
    "    duration_preservation.append(preservation)\n",
    "\n",
    "volume_preservation = []\n",
    "for cls in final_volume_dist.index:\n",
    "    original_ratio = volume_dist[cls] / len(flow_data)\n",
    "    sampled_ratio = final_volume_dist[cls] / len(sampled_flow_data)\n",
    "    preservation = min(sampled_ratio/original_ratio, original_ratio/sampled_ratio) * 100\n",
    "    volume_preservation.append(preservation)\n",
    "\n",
    "avg_duration_preservation = np.mean(duration_preservation)\n",
    "avg_volume_preservation = np.mean(volume_preservation)\n",
    "overall_preservation = (avg_duration_preservation + avg_volume_preservation) / 2\n",
    "\n",
    "print(f\"\\nâš–ï¸ ë³µí•© ì¸µí™” ë¹„ìœ¨ ë³´ì¡´ë„:\")\n",
    "print(f\"   Duration: {avg_duration_preservation:.1f}%\")\n",
    "print(f\"   Volume: {avg_volume_preservation:.1f}%\") \n",
    "print(f\"   ì „ì²´ í‰ê· : {overall_preservation:.1f}% (100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½)\")\n",
    "\n",
    "print(f\"\\nâœ… ë³µí•© ì¸µí™” ìƒ˜í”Œë§ ì™„ë£Œ! Duration+Volume ë™ì‹œ ë¹„ìœ¨ ë³´ì¡´ìœ¼ë¡œ ìµœê³  í’ˆì§ˆ ìƒ˜í”Œ í™•ë³´! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f367f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ 5ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§\n",
      "==================================================\n",
      "ğŸš€ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_50000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_100000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_100000.pkl\n",
      "      ì§„í–‰ë¥ : 5,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 5,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_150000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_150000.pkl\n",
      "      ì§„í–‰ë¥ : 10,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 10,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_250000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_250000.pkl\n",
      "      ì§„í–‰ë¥ : 15,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 15,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_300000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_300000.pkl\n",
      "      ì§„í–‰ë¥ : 20,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 20,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_350000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_350000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_400000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_400000.pkl\n",
      "      ì§„í–‰ë¥ : 25,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 25,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_450000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_450000.pkl\n",
      "      ì§„í–‰ë¥ : 30,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 30,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_500000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_500000.pkl\n",
      "      ì§„í–‰ë¥ : 35,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 35,000ê°œ ì™„ë£Œ\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_600000.pkl\n",
      "   ğŸ“‚ ë¡œë”©: task2_data/train_packet_data_600000.pkl\n",
      "      ì§„í–‰ë¥ : 40,000ê°œ ì™„ë£Œ\n",
      "      ì§„í–‰ë¥ : 40,000ê°œ ì™„ë£Œ\n",
      "\n",
      "âœ… ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!\n",
      "ğŸ“Š ì¶”ì¶œëœ íŠ¹ì§• ìˆ˜: 41,660ê°œ\n",
      "ğŸ¯ ìœ íš¨ ì¸ë±ìŠ¤ ìˆ˜: 41,660ê°œ\n",
      "\n",
      "âœ… ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!\n",
      "ğŸ“Š ì¶”ì¶œëœ íŠ¹ì§• ìˆ˜: 41,660ê°œ\n",
      "ğŸ¯ ìœ íš¨ ì¸ë±ìŠ¤ ìˆ˜: 41,660ê°œ\n",
      "âœ“ ìµœì¢… íŠ¹ì§• í–‰ë ¬ í¬ê¸°: (41660, 44)\n",
      "âœ“ íƒ€ê²Ÿ ë°ì´í„° í¬ê¸°: (41660, 11)\n",
      "\n",
      "ğŸŒŸ ìƒì„±ëœ ê³ ê¸‰ íŠ¹ì§•ë“¤ (18ê°œ):\n",
      "    1. ğŸ§  ip_len_mean_13\n",
      "    2. ğŸ§  ip_len_std_13\n",
      "    3. ğŸ§  ip_len_max_13\n",
      "    4. ğŸ§  ip_len_min_13\n",
      "    5. ğŸ§  ip_len_range_13\n",
      "    6. ğŸ§  ip_len_median_13\n",
      "    7. ğŸ§  ip_len_trend\n",
      "    8. ğŸ§  ip_len_volatility\n",
      "    9. ğŸ§  inter_time_mean_13\n",
      "   10. ğŸ§  inter_time_std_13\n",
      "   ... ì™¸ 8ê°œ ë”\n",
      "\n",
      "âœ… 5ë‹¨ê³„ ì™„ë£Œ: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ! ğŸ‰\n",
      "âœ“ ìµœì¢… íŠ¹ì§• í–‰ë ¬ í¬ê¸°: (41660, 44)\n",
      "âœ“ íƒ€ê²Ÿ ë°ì´í„° í¬ê¸°: (41660, 11)\n",
      "\n",
      "ğŸŒŸ ìƒì„±ëœ ê³ ê¸‰ íŠ¹ì§•ë“¤ (18ê°œ):\n",
      "    1. ğŸ§  ip_len_mean_13\n",
      "    2. ğŸ§  ip_len_std_13\n",
      "    3. ğŸ§  ip_len_max_13\n",
      "    4. ğŸ§  ip_len_min_13\n",
      "    5. ğŸ§  ip_len_range_13\n",
      "    6. ğŸ§  ip_len_median_13\n",
      "    7. ğŸ§  ip_len_trend\n",
      "    8. ğŸ§  ip_len_volatility\n",
      "    9. ğŸ§  inter_time_mean_13\n",
      "   10. ğŸ§  inter_time_std_13\n",
      "   ... ì™¸ 8ê°œ ë”\n",
      "\n",
      "âœ… 5ë‹¨ê³„ ì™„ë£Œ: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ! ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# 5ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ (ìµœëŒ€ 3ê°œ íŒ¨í‚· í™œìš©)\n",
    "print(f\"\\nğŸ”§ 5ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def extract_advanced_features(sampled_indices, packet_files):\n",
    "    \"\"\"\n",
    "    ìƒ˜í”Œë§ëœ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\")\n",
    "    \n",
    "    features_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for global_idx in sampled_indices:\n",
    "        try:\n",
    "            # ì–´ë–¤ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¬ì§€ ê²°ì •\n",
    "            file_idx = global_idx // 50000\n",
    "            local_idx = global_idx % 50000\n",
    "            \n",
    "            if file_idx >= len(packet_files):\n",
    "                continue\n",
    "                \n",
    "            # í•´ë‹¹ íŒŒì¼ ë¡œë”© (ìºì‹œ í™œìš©)\n",
    "            file_path = packet_files[file_idx]\n",
    "            \n",
    "            if not hasattr(extract_advanced_features, 'cache'):\n",
    "                extract_advanced_features.cache = {}\n",
    "            \n",
    "            if file_path not in extract_advanced_features.cache:\n",
    "                print(f\"   ğŸ“‚ ë¡œë”©: {file_path}\")\n",
    "                extract_advanced_features.cache[file_path] = joblib.load(file_path)\n",
    "            \n",
    "            packet_data = extract_advanced_features.cache[file_path]\n",
    "            \n",
    "            # ë¡œì»¬ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ìŠ¤í‚µ\n",
    "            if local_idx >= len(packet_data):\n",
    "                continue\n",
    "                \n",
    "            packet_df = packet_data[local_idx]\n",
    "            valid_packets = packet_df.dropna()\n",
    "            \n",
    "            if len(valid_packets) >= 1:  # ìµœì†Œ 1ê°œ íŒ¨í‚· í•„ìš”\n",
    "                features = extract_packet_features(valid_packets)\n",
    "                features_list.append(features)\n",
    "                valid_indices.append(global_idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        if len(features_list) % 5000 == 0:\n",
    "            print(f\"      ì§„í–‰ë¥ : {len(features_list):,}ê°œ ì™„ë£Œ\")\n",
    "    \n",
    "    return features_list, valid_indices\n",
    "\n",
    "def extract_packet_features(packets):\n",
    "    \"\"\"\n",
    "    íŒ¨í‚·ë“¤ë¡œë¶€í„° ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ (ìµœëŒ€ 3ê°œ íŒ¨í‚·ë§Œ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    num_packets = min(3, len(packets))\n",
    "    packets = packets.iloc[:num_packets]\n",
    "    \n",
    "    # ìˆ«ìí˜• ì»¬ëŸ¼ í™•ì¸\n",
    "    numeric_cols = packets.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # === ê¸°ë³¸ íŠ¹ì§•ë“¤ ===\n",
    "    for col in numeric_cols:\n",
    "        features[f'first_{col}'] = packets.iloc[0][col] if col in packets.columns else 0\n",
    "        features[f'second_{col}'] = packets.iloc[1][col] if col in packets.columns and len(packets) > 1 else 0\n",
    "    \n",
    "    # === ğŸŒŸ í†µê³„ íŠ¹ì§• (1~3ê°œ íŒ¨í‚·) ===\n",
    "    if 'ip_len' in packets.columns:\n",
    "        ip_lens = packets['ip_len'].values\n",
    "        features['ip_len_mean_13'] = np.mean(ip_lens)\n",
    "        features['ip_len_std_13'] = np.std(ip_lens) if len(ip_lens) > 1 else 0\n",
    "        features['ip_len_max_13'] = np.max(ip_lens)\n",
    "        features['ip_len_min_13'] = np.min(ip_lens)\n",
    "        features['ip_len_range_13'] = np.max(ip_lens) - np.min(ip_lens)\n",
    "        features['ip_len_median_13'] = np.median(ip_lens)\n",
    "        \n",
    "        # ë³€í™” íŒ¨í„´\n",
    "        if len(ip_lens) >= 3:\n",
    "            diffs = np.diff(ip_lens)\n",
    "            features['ip_len_trend'] = 1 if np.mean(diffs) > 0 else (-1 if np.mean(diffs) < 0 else 0)\n",
    "            features['ip_len_volatility'] = np.std(diffs) if len(diffs) > 1 else 0\n",
    "        else:\n",
    "            features['ip_len_trend'] = 0\n",
    "            features['ip_len_volatility'] = 0\n",
    "    \n",
    "    # íŒ¨í‚· ê°„ ì‹œê°„ í†µê³„\n",
    "    if 'packet_capture_time' in packets.columns:\n",
    "        try:\n",
    "            times = pd.to_datetime(packets['packet_capture_time'])\n",
    "            time_diffs = np.diff(times).astype('timedelta64[us]').astype(float)\n",
    "            \n",
    "            if len(time_diffs) > 0:\n",
    "                features['inter_time_mean_13'] = np.mean(time_diffs)\n",
    "                features['inter_time_std_13'] = np.std(time_diffs) if len(time_diffs) > 1 else 0\n",
    "                features['inter_time_max_13'] = np.max(time_diffs)\n",
    "                features['inter_time_min_13'] = np.min(time_diffs)\n",
    "                features['timing_consistency'] = np.std(time_diffs) / (np.mean(time_diffs) + 1)\n",
    "            else:\n",
    "                for key in ['inter_time_mean_13', 'inter_time_std_13', 'inter_time_max_13', \n",
    "                           'inter_time_min_13', 'timing_consistency']:\n",
    "                    features[key] = 0\n",
    "        except:\n",
    "            for key in ['inter_time_mean_13', 'inter_time_std_13', 'inter_time_max_13', \n",
    "                       'inter_time_min_13', 'timing_consistency']:\n",
    "                features[key] = 0\n",
    "    \n",
    "    # TCP íš¨ìœ¨ì„±\n",
    "    if 'tcp_len' in packets.columns and 'ip_len' in packets.columns:\n",
    "        tcp_lens = packets['tcp_len'].values\n",
    "        features['tcp_len_mean_13'] = np.mean(tcp_lens)\n",
    "        features['tcp_len_std_13'] = np.std(tcp_lens) if len(tcp_lens) > 1 else 0\n",
    "        features['tcp_len_sum_13'] = np.sum(tcp_lens)\n",
    "        \n",
    "        total_ip = np.sum(packets['ip_len'])\n",
    "        total_tcp = np.sum(tcp_lens)\n",
    "        features['tcp_efficiency_13'] = total_tcp / max(total_ip, 1)\n",
    "    \n",
    "    # === ğŸŒŸ TCP í”Œë˜ê·¸ íŒ¨í„´ íŠ¹ì§• ===\n",
    "    if 'tcp_flags' in packets.columns:\n",
    "        flags = packets['tcp_flags'].values\n",
    "        \n",
    "        # ê°œë³„ í”Œë˜ê·¸\n",
    "        features['has_syn'] = int(any(flag & 0x02 for flag in flags))\n",
    "        features['has_ack'] = int(any(flag & 0x10 for flag in flags))\n",
    "        features['has_fin'] = int(any(flag & 0x01 for flag in flags))\n",
    "        features['has_rst'] = int(any(flag & 0x04 for flag in flags))\n",
    "        features['has_psh'] = int(any(flag & 0x08 for flag in flags))\n",
    "        \n",
    "        # í•¸ë“œì…°ì´í¬ ì™„ì „ì„±\n",
    "        if len(flags) >= 3:\n",
    "            first_syn = (flags[0] & 0x02) != 0\n",
    "            second_syn_ack = (flags[1] & 0x12) == 0x12\n",
    "            third_ack = (flags[2] & 0x10) != 0\n",
    "            features['is_handshake_complete'] = int(first_syn and second_syn_ack and third_ack)\n",
    "            \n",
    "            has_fin_ack = any((flag & 0x11) == 0x11 for flag in flags)\n",
    "            features['is_graceful_close'] = int(has_fin_ack)\n",
    "        else:\n",
    "            features['is_handshake_complete'] = 0\n",
    "            features['is_graceful_close'] = 0\n",
    "        \n",
    "        features['flag_diversity'] = len(set(flags))\n",
    "        psh_count = sum(1 for flag in flags if flag & 0x08)\n",
    "        features['push_frequency'] = psh_count / len(flags)\n",
    "    \n",
    "    # === ğŸŒŸ ì¶”ê°€ ê³ ê¸‰ íŠ¹ì§•ë“¤ ===\n",
    "    # í¬ê¸° ë³€í™” íŒ¨í„´\n",
    "    if 'ip_len' in packets.columns and len(packets) >= 3:\n",
    "        sizes = packets['ip_len'].values\n",
    "        increases = sum(1 for i in range(1, len(sizes)) if sizes[i] > sizes[i-1])\n",
    "        decreases = sum(1 for i in range(1, len(sizes)) if sizes[i] < sizes[i-1])\n",
    "        \n",
    "        features['size_increase_count'] = increases\n",
    "        features['size_decrease_count'] = decreases\n",
    "        features['size_stability'] = sum(1 for i in range(1, len(sizes)) if sizes[i] == sizes[i-1])\n",
    "    \n",
    "    # ë¹„ìœ¨ íŠ¹ì§•\n",
    "    if 'tcp_len' in packets.columns and 'ip_len' in packets.columns:\n",
    "        tcp_to_ip_ratio_1 = packets.iloc[0]['tcp_len'] / max(packets.iloc[0]['ip_len'], 1)\n",
    "        features['tcp_to_ip_ratio_first'] = tcp_to_ip_ratio_1\n",
    "        \n",
    "        if len(packets) > 1:\n",
    "            tcp_to_ip_ratio_2 = packets.iloc[1]['tcp_len'] / max(packets.iloc[1]['ip_len'], 1)\n",
    "            features['tcp_to_ip_ratio_second'] = tcp_to_ip_ratio_2\n",
    "    \n",
    "    # ê¸°ì¡´ í˜¸í™˜ì„± íŠ¹ì§•\n",
    "    features['inter_packet_time_us'] = features.get('inter_time_mean_13', 0)\n",
    "    features['ip_len_diff'] = features.get('second_ip_len', 0) - features.get('first_ip_len', 0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì‹¤í–‰\n",
    "advanced_features_list, valid_global_indices = extract_advanced_features(sampled_indices, packet_files)\n",
    "\n",
    "print(f\"\\nâœ… ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ì¶”ì¶œëœ íŠ¹ì§• ìˆ˜: {len(advanced_features_list):,}ê°œ\")\n",
    "print(f\"ğŸ¯ ìœ íš¨ ì¸ë±ìŠ¤ ìˆ˜: {len(valid_global_indices):,}ê°œ\")\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "advanced_features_df = pd.DataFrame(advanced_features_list)\n",
    "advanced_features_df = advanced_features_df.fillna(0)\n",
    "\n",
    "# ëŒ€ì‘í•˜ëŠ” íƒ€ê²Ÿ ë°ì´í„°\n",
    "valid_flow_data = flow_data.iloc[valid_global_indices].copy()\n",
    "valid_flow_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"âœ“ ìµœì¢… íŠ¹ì§• í–‰ë ¬ í¬ê¸°: {advanced_features_df.shape}\")\n",
    "print(f\"âœ“ íƒ€ê²Ÿ ë°ì´í„° í¬ê¸°: {valid_flow_data.shape}\")\n",
    "\n",
    "# ìƒˆë¡œìš´ ê³ ê¸‰ íŠ¹ì§•ë“¤ í™•ì¸\n",
    "new_advanced_features = [col for col in advanced_features_df.columns \n",
    "                        if any(pattern in col for pattern in ['_13', 'handshake', 'efficiency', \n",
    "                                                             'consistency', 'diversity', 'frequency',\n",
    "                                                             'stability', 'trend', 'volatility'])]\n",
    "\n",
    "print(f\"\\nğŸŒŸ ìƒì„±ëœ ê³ ê¸‰ íŠ¹ì§•ë“¤ ({len(new_advanced_features)}ê°œ):\")\n",
    "for i, feat in enumerate(new_advanced_features[:10], 1):  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ\n",
    "    print(f\"   {i:2d}. ğŸ§  {feat}\")\n",
    "if len(new_advanced_features) > 10:\n",
    "    print(f\"   ... ì™¸ {len(new_advanced_features)-10}ê°œ ë”\")\n",
    "\n",
    "print(f\"\\nâœ… 5ë‹¨ê³„ ì™„ë£Œ: ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ! ğŸ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ 6ë‹¨ê³„: 5ê°œ ëª¨ë¸ 100íšŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
      "================================================================================\n",
      "âœ… Optunaë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-07 23:52:59,064] A new study created in memory with name: no-name-793dd8bb-e312-4f41-afcd-3ec8e2894613\n",
      "[W 2025-08-07 23:52:59,067] Trial 0 failed with parameters: {'iterations': 300, 'depth': 10, 'learning_rate': 0.03, 'l2_leaf_reg': 10, 'border_count': 254, 'bagging_temperature': 0.5, 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'} because of the following error: TypeError(\"got an unexpected keyword argument 'fit_params'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hg226\\AppData\\Local\\Temp\\ipykernel_5696\\2007194405.py\", line 185, in objective\n",
      "    scores = cross_val_score(\n",
      "        model, self.X, self.y, cv=self.cv, scoring=self.scoring,\n",
      "        fit_params=self.fit_params if hasattr(self, 'fit_params') else None,\n",
      "        n_jobs=1\n",
      "    )\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 194, in wrapper\n",
      "    params = func_sig.bind(*args, **kwargs)\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3264, in bind\n",
      "    return self._bind(args, kwargs)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3253, in _bind\n",
      "    raise TypeError(\n",
      "        'got an unexpected keyword argument {arg!r}'.format(\n",
      "            arg=next(iter(kwargs))))\n",
      "TypeError: got an unexpected keyword argument 'fit_params'\n",
      "[W 2025-08-07 23:52:59,115] Trial 0 failed with value None.\n",
      "[W 2025-08-07 23:52:59,067] Trial 0 failed with parameters: {'iterations': 300, 'depth': 10, 'learning_rate': 0.03, 'l2_leaf_reg': 10, 'border_count': 254, 'bagging_temperature': 0.5, 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'} because of the following error: TypeError(\"got an unexpected keyword argument 'fit_params'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hg226\\AppData\\Local\\Temp\\ipykernel_5696\\2007194405.py\", line 185, in objective\n",
      "    scores = cross_val_score(\n",
      "        model, self.X, self.y, cv=self.cv, scoring=self.scoring,\n",
      "        fit_params=self.fit_params if hasattr(self, 'fit_params') else None,\n",
      "        n_jobs=1\n",
      "    )\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 194, in wrapper\n",
      "    params = func_sig.bind(*args, **kwargs)\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3264, in bind\n",
      "    return self._bind(args, kwargs)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3253, in _bind\n",
      "    raise TypeError(\n",
      "        'got an unexpected keyword argument {arg!r}'.format(\n",
      "            arg=next(iter(kwargs))))\n",
      "TypeError: got an unexpected keyword argument 'fit_params'\n",
      "[W 2025-08-07 23:52:59,115] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ GPU ìƒíƒœ:\n",
      "   NVIDIA GPU: âœ…\n",
      "   CatBoost GPU: âœ…\n",
      "   XGBoost GPU: âœ…\n",
      "   LightGBM GPU: âœ…\n",
      "\n",
      "ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\n",
      "   í•™ìŠµ ë°ì´í„°: 33,328ê°œ\n",
      "   ê²€ì¦ ë°ì´í„°: 8,332ê°œ\n",
      "   íŠ¹ì§• ìˆ˜: 44ê°œ\n",
      "\n",
      "âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:\n",
      "   Duration: {0: np.float64(6.345773038842346), 1: np.float64(0.701405842242613), 2: np.float64(1.0609957977842863), 3: np.float64(0.678335911422291)}\n",
      "   Volume: {0: np.float64(0.723389477339816), 1: np.float64(1.0242163491087892), 2: np.float64(0.7099522835719154), 3: np.float64(4.29706034038164)}\n",
      "\n",
      "================================================================================\n",
      "ğŸ± 1. CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ)\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ CatBoost - Duration ë¶„ë¥˜ íŠœë‹...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 364\u001b[39m\n\u001b[32m    352\u001b[39m start_time = time.time()\n\u001b[32m    354\u001b[39m catboost_search_duration = OptunaHyperparameterSearch(\n\u001b[32m    355\u001b[39m     model_class=CatBoostClassifier,\n\u001b[32m    356\u001b[39m     param_ranges=catboost_param_ranges,\n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m     gpu_available=gpu_status[\u001b[33m'\u001b[39m\u001b[33mcatboost\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    362\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43mcatboost_search_duration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_duration_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration_class_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ±ï¸ ì†Œìš”ì‹œê°„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mì´ˆ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ† ìµœì  íŒŒë¼ë¯¸í„°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatboost_search_duration.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 220\u001b[39m, in \u001b[36mOptunaHyperparameterSearch.fit\u001b[39m\u001b[34m(self, X, y, class_weights)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ì§„í–‰ë¥ : (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | ìµœê³ : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m study = optuna.create_study(\n\u001b[32m    216\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    217\u001b[39m     sampler=optuna.samplers.TPESampler(seed=\u001b[38;5;28mself\u001b[39m.random_state)\n\u001b[32m    218\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mself\u001b[39m.best_params_ = study.best_params\n\u001b[32m    228\u001b[39m \u001b[38;5;28mself\u001b[39m.best_score_ = study.best_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mOptunaHyperparameterSearch.objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# êµì°¨ ê²€ì¦\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfit_params\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3264\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3261\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3262\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3253\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3243\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3244\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3245\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3250\u001b[39m             ),\n\u001b[32m   3251\u001b[39m         )\n\u001b[32m   3252\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3254\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3255\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'fit_params'"
     ]
    }
   ],
   "source": [
    "# 6ë‹¨ê³„: 5ê°œ ëª¨ë¸ 100íšŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (GPU ê°€ì† + í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ + Optuna)\n",
    "print(f\"\\nğŸš€ 6ë‹¨ê³„: 5ê°œ ëª¨ë¸ 100íšŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "\n",
    "# Optuna ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "try:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)  # ì§„í–‰ìƒí™© í‘œì‹œ\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"âœ… Optunaë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"âŒ Optuna ì—†ìŒ - ê¸°ì¡´ ëœë¤ ì„œì¹˜ ì‚¬ìš©\")\n",
    "    print(\"   ì„¤ì¹˜: pip install optuna\")\n",
    "\n",
    "# GPU í™•ì¸ í•¨ìˆ˜ (ê°•í™” ë²„ì „)\n",
    "def check_gpu_advanced():\n",
    "    gpu_status = {}\n",
    "    \n",
    "    # NVIDIA GPU í™•ì¸\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
    "        gpu_status['nvidia'] = result.returncode == 0\n",
    "    except:\n",
    "        gpu_status['nvidia'] = False\n",
    "    \n",
    "    # CatBoost GPU í…ŒìŠ¤íŠ¸\n",
    "    try:\n",
    "        test_cat = CatBoostClassifier(task_type='GPU', iterations=1, verbose=False)\n",
    "        test_cat.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['catboost'] = True\n",
    "    except:\n",
    "        gpu_status['catboost'] = False\n",
    "    \n",
    "    # XGBoost GPU í…ŒìŠ¤íŠ¸\n",
    "    try:\n",
    "        test_xgb = xgb.XGBClassifier(tree_method='gpu_hist', n_estimators=1)\n",
    "        test_xgb.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['xgboost'] = True\n",
    "    except:\n",
    "        gpu_status['xgboost'] = False\n",
    "    \n",
    "    # LightGBM GPU í…ŒìŠ¤íŠ¸\n",
    "    try:\n",
    "        test_lgb = lgb.LGBMClassifier(device='gpu', n_estimators=1, verbose=-1)\n",
    "        test_lgb.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['lightgbm'] = True\n",
    "    except:\n",
    "        gpu_status['lightgbm'] = False\n",
    "    \n",
    "    return gpu_status\n",
    "\n",
    "gpu_status = check_gpu_advanced()\n",
    "print(f\"ğŸ–¥ï¸ GPU ìƒíƒœ:\")\n",
    "print(f\"   NVIDIA GPU: {'âœ…' if gpu_status['nvidia'] else 'âŒ'}\")\n",
    "print(f\"   CatBoost GPU: {'âœ…' if gpu_status['catboost'] else 'âŒ'}\")\n",
    "print(f\"   XGBoost GPU: {'âœ…' if gpu_status['xgboost'] else 'âŒ'}\")\n",
    "print(f\"   LightGBM GPU: {'âœ…' if gpu_status['lightgbm'] else 'âŒ'}\")\n",
    "\n",
    "# ë°ì´í„° ì¤€ë¹„\n",
    "X = advanced_features_df\n",
    "y_duration = valid_flow_data['duration_class']\n",
    "y_volume = valid_flow_data['volume_class']\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_duration_train, y_duration_test = train_test_split(\n",
    "    X, y_duration, test_size=0.2, random_state=42, stratify=y_duration\n",
    ")\n",
    "_, _, y_volume_train, y_volume_test = train_test_split(\n",
    "    X, y_volume, test_size=0.2, random_state=42, stratify=y_volume\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:\")\n",
    "print(f\"   í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ\")\n",
    "print(f\"   ê²€ì¦ ë°ì´í„°: {X_test.shape[0]:,}ê°œ\") \n",
    "print(f\"   íŠ¹ì§• ìˆ˜: {X_train.shape[1]:,}ê°œ\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "duration_classes = np.unique(y_duration_train)\n",
    "duration_weights = compute_class_weight('balanced', classes=duration_classes, y=y_duration_train)\n",
    "duration_class_weights = {int(cls): weight for cls, weight in zip(duration_classes, duration_weights)}\n",
    "\n",
    "volume_classes = np.unique(y_volume_train)\n",
    "volume_weights = compute_class_weight('balanced', classes=volume_classes, y=y_volume_train)\n",
    "volume_class_weights = {int(cls): weight for cls, weight in zip(volume_classes, volume_weights)}\n",
    "\n",
    "print(f\"\\nâš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:\")\n",
    "print(f\"   Duration: {duration_class_weights}\")\n",
    "print(f\"   Volume: {volume_class_weights}\")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "results = {\n",
    "    'model': [], 'task': [], 'accuracy': [], 'precision': [], \n",
    "    'recall': [], 'f1_weighted': [], 'f1_macro': [], 'best_params': []\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, task_name, best_params):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    results['model'].append(model_name)\n",
    "    results['task'].append(task_name)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1_weighted'].append(f1_weighted)\n",
    "    results['f1_macro'].append(f1_macro)\n",
    "    results['best_params'].append(str(best_params)[:100])  # ê¸¸ì´ ì œí•œ\n",
    "    \n",
    "    print(f\"   ì •í™•ë„: {accuracy:.4f} | ì •ë°€ë„: {precision:.4f} | ì¬í˜„ìœ¨: {recall:.4f}\")\n",
    "    print(f\"   F1(weighted): {f1_weighted:.4f} | F1(macro): {f1_macro:.4f}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def create_sample_weights(y, class_weights):\n",
    "    \"\"\"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ ë°°ì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    return np.array([class_weights[cls] for cls in y])\n",
    "\n",
    "# Optuna ê¸°ë°˜ ì§€ëŠ¥í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í´ë˜ìŠ¤\n",
    "class OptunaHyperparameterSearch:\n",
    "    def __init__(self, model_class, param_ranges, n_trials, cv, scoring, random_state, gpu_available=False):\n",
    "        self.model_class = model_class\n",
    "        self.param_ranges = param_ranges\n",
    "        self.n_trials = n_trials\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.random_state = random_state\n",
    "        self.gpu_available = gpu_available\n",
    "        self.best_estimator_ = None\n",
    "        self.best_params_ = None\n",
    "        self.best_score_ = None\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        # íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§ (Optuna ê¶Œì¥ ë°©ì‹: ì´ì‚°ê°’ì€ categoricalë¡œ ì²˜ë¦¬)\n",
    "        params = {}\n",
    "        for param_name, param_config in self.param_ranges.items():\n",
    "            if isinstance(param_config, list):\n",
    "                # ğŸ”¬ í”„ë¡œ íŒ: ëª¨ë“  ë¦¬ìŠ¤íŠ¸ëŠ” categoricalë¡œ ì²˜ë¦¬í•˜ì—¬ ì •í™•í•œ íƒìƒ‰ ê³µê°„ ì œì–´\n",
    "                params[param_name] = trial.suggest_categorical(param_name, param_config)\n",
    "        \n",
    "        # ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •\n",
    "        model_params = {}\n",
    "        if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "            model_params.update({\n",
    "                'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                'random_seed': self.random_state,\n",
    "                'verbose': False,\n",
    "                'eval_metric': 'MultiClass'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'XGBClassifier':\n",
    "            model_params.update({\n",
    "                'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                'eval_metric': 'mlogloss'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "            model_params.update({\n",
    "                'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                'random_state': self.random_state,\n",
    "                'verbose': -1,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1\n",
    "            })\n",
    "        elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "            model_params.update({\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1\n",
    "            })\n",
    "        \n",
    "        # ëª¨ë¸ ìƒì„±\n",
    "        model = self.model_class(**{**model_params, **params})\n",
    "        \n",
    "        # êµì°¨ ê²€ì¦ - XGBoost sample_weight ë¬¸ì œ í•´ê²°\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        from sklearn.metrics import f1_score\n",
    "        import numpy as np\n",
    "        \n",
    "        # ìˆ˜ë™ êµì°¨ ê²€ì¦ìœ¼ë¡œ sample_weight ë¬¸ì œ í•´ê²°\n",
    "        skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(self.X, self.y):\n",
    "            X_train_fold, X_val_fold = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = self.y.iloc[train_idx], self.y.iloc[val_idx]\n",
    "            \n",
    "            # ëª¨ë¸ í•™ìŠµ\n",
    "            if hasattr(self, 'fit_params') and self.fit_params:\n",
    "                # XGBoostì˜ ê²½ìš° sample_weight ì ìš©\n",
    "                if 'sample_weight' in self.fit_params:\n",
    "                    fold_sample_weights = self.fit_params['sample_weight'][train_idx]\n",
    "                    model.fit(X_train_fold, y_train_fold, sample_weight=fold_sample_weights)\n",
    "                else:\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "            else:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # ì˜ˆì¸¡ ë° ì ìˆ˜ ê³„ì‚°\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            score = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def fit(self, X, y, class_weights=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì²˜ë¦¬\n",
    "        self.fit_params = {}\n",
    "        if class_weights is not None:\n",
    "            if self.model_class.__name__ == 'XGBClassifier':\n",
    "                # XGBoostëŠ” sample_weight ì‚¬ìš©\n",
    "                sample_weights = create_sample_weights(y, class_weights)\n",
    "                self.fit_params = {'sample_weight': sample_weights}\n",
    "            elif self.model_class.__name__ == 'CatBoostClassifier':\n",
    "                # CatBoostëŠ” class_weights íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
    "                self.class_weights = class_weights\n",
    "            else:\n",
    "                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ class_weight='balanced' ì‚¬ìš©\n",
    "                pass\n",
    "        \n",
    "        if OPTUNA_AVAILABLE:\n",
    "            # Optuna ì‚¬ìš© - ì§„í–‰ìƒí™© ì½œë°± ì¶”ê°€\n",
    "            def progress_callback(study, trial):\n",
    "                print(f\"   ì§„í–‰ë¥ : ({trial.number + 1:3d}/{self.n_trials}) F1: {trial.value:.4f} | ìµœê³ : {study.best_value:.4f}\")\n",
    "            \n",
    "            study = optuna.create_study(\n",
    "                direction='maximize',\n",
    "                sampler=optuna.samplers.TPESampler(seed=self.random_state)\n",
    "            )\n",
    "            \n",
    "            study.optimize(\n",
    "                self.objective, \n",
    "                n_trials=self.n_trials, \n",
    "                callbacks=[progress_callback],\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            self.best_params_ = study.best_params\n",
    "            self.best_score_ = study.best_value\n",
    "        else:\n",
    "            # ê¸°ì¡´ ëœë¤ ì„œì¹˜ ì‚¬ìš© - ì§„í–‰ìƒí™© í‘œì‹œ ì¶”ê°€\n",
    "            from sklearn.model_selection import ParameterSampler\n",
    "            param_list = list(ParameterSampler(self.param_ranges, n_iter=self.n_trials, random_state=self.random_state))\n",
    "            \n",
    "            best_score = -np.inf\n",
    "            best_params = None\n",
    "            \n",
    "            for i, params in enumerate(param_list, 1):\n",
    "                print(f\"   ì§„í–‰ë¥ : ({i:3d}/{self.n_trials}) í…ŒìŠ¤íŠ¸ ì¤‘...\", end=' ')\n",
    "                \n",
    "                try:\n",
    "                    model_params = {}\n",
    "                    if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "                        model_params.update({\n",
    "                            'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                            'random_seed': self.random_state,\n",
    "                            'verbose': False,\n",
    "                            'eval_metric': 'MultiClass'\n",
    "                        })\n",
    "                    elif self.model_class.__name__ == 'XGBClassifier':\n",
    "                        model_params.update({\n",
    "                            'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                            'random_state': self.random_state,\n",
    "                            'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                            'eval_metric': 'mlogloss'\n",
    "                        })\n",
    "                    elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "                        model_params.update({\n",
    "                            'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                            'random_state': self.random_state,\n",
    "                            'verbose': -1,\n",
    "                            'n_jobs': -1 if not self.gpu_available else 1\n",
    "                        })\n",
    "                    elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "                        model_params.update({\n",
    "                            'random_state': self.random_state,\n",
    "                            'n_jobs': -1\n",
    "                        })\n",
    "                    \n",
    "                    model = self.model_class(**{**model_params, **params})\n",
    "                    \n",
    "                    # ìˆ˜ë™ êµì°¨ ê²€ì¦ìœ¼ë¡œ sample_weight ë¬¸ì œ í•´ê²°\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.metrics import f1_score\n",
    "                    import numpy as np\n",
    "                    \n",
    "                    skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n",
    "                    scores = []\n",
    "                    \n",
    "                    for train_idx, val_idx in skf.split(X, y):\n",
    "                        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                        \n",
    "                        # ê° ëª¨ë¸ë³„ ë³µì‚¬ë³¸ ìƒì„±\n",
    "                        fold_model = self.model_class(**{**model_params, **params})\n",
    "                        \n",
    "                        # ëª¨ë¸ í•™ìŠµ\n",
    "                        if hasattr(self, 'fit_params') and self.fit_params:\n",
    "                            # XGBoostì˜ ê²½ìš° sample_weight ì ìš©\n",
    "                            if 'sample_weight' in self.fit_params:\n",
    "                                fold_sample_weights = self.fit_params['sample_weight'][train_idx]\n",
    "                                fold_model.fit(X_train_fold, y_train_fold, sample_weight=fold_sample_weights)\n",
    "                            else:\n",
    "                                fold_model.fit(X_train_fold, y_train_fold)\n",
    "                        else:\n",
    "                            fold_model.fit(X_train_fold, y_train_fold)\n",
    "                        \n",
    "                        # ì˜ˆì¸¡ ë° ì ìˆ˜ ê³„ì‚°\n",
    "                        y_pred_fold = fold_model.predict(X_val_fold)\n",
    "                        score = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "                        scores.append(score)\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    print(f\"F1: {avg_score:.4f} | ìµœê³ : {best_score:.4f}\")\n",
    "                    \n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = params\n",
    "                except Exception as e:\n",
    "                    print(f\"ì‹¤íŒ¨\")\n",
    "                    continue\n",
    "            \n",
    "            self.best_params_ = best_params\n",
    "            self.best_score_ = best_score\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "        model_params = {}\n",
    "        if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "            model_params.update({\n",
    "                'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                'class_weights': class_weights if class_weights else None,\n",
    "                'random_seed': self.random_state,\n",
    "                'verbose': False,\n",
    "                'eval_metric': 'MultiClass'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'XGBClassifier':\n",
    "            model_params.update({\n",
    "                'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                'eval_metric': 'mlogloss'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "            model_params.update({\n",
    "                'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                'verbose': -1,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1\n",
    "            })\n",
    "        elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "            model_params.update({\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1\n",
    "            })\n",
    "        \n",
    "        self.best_estimator_ = self.model_class(**{**model_params, **self.best_params_})\n",
    "        \n",
    "        # XGBoostì˜ ê²½ìš° sample_weightì™€ í•¨ê»˜ í•™ìŠµ\n",
    "        if self.model_class.__name__ == 'XGBClassifier' and class_weights:\n",
    "            sample_weights = create_sample_weights(y, class_weights)\n",
    "            self.best_estimator_.fit(X, y, sample_weight=sample_weights)\n",
    "        else:\n",
    "            self.best_estimator_.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ± 1. CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸ”¬ í”„ë¡œ íŒ: ì´ì‚°ê°’ë“¤ì€ ì •í™•íˆ ì§€ì •ëœ ê°’ë“¤ë§Œ íƒìƒ‰í•˜ë„ë¡ ì„¤ê³„\n",
    "catboost_param_ranges = {\n",
    "    'iterations': [200, 300, 400, 500, 600],           # ì •í™•íˆ ì´ ê°’ë“¤ë§Œ íƒìƒ‰\n",
    "    'depth': [6, 8, 10, 12, 14],                       # 7, 9, 11, 13ì€ ì œì™¸\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15],  # ì‹¤í—˜ì—ì„œ ê²€ì¦ëœ ê°’ë“¤ë§Œ\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 10, 15],\n",
    "    'border_count': [64, 128, 254],                    # CatBoost ê¶Œì¥ê°’ë“¤ë§Œ\n",
    "    'bagging_temperature': [0.5, 1.0, 1.5, 2.0],\n",
    "    'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],\n",
    "    'leaf_estimation_method': ['Newton', 'Gradient']\n",
    "}\n",
    "\n",
    "# Duration ë¶„ë¥˜\n",
    "print(\"\\nğŸ¯ CatBoost - Duration ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "catboost_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=CatBoostClassifier,\n",
    "    param_ranges=catboost_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['catboost']\n",
    ")\n",
    "\n",
    "catboost_search_duration.fit(X_train, y_duration_train, class_weights=duration_class_weights)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {catboost_search_duration.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {catboost_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(catboost_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'CatBoost', 'Duration', catboost_search_duration.best_params_)\n",
    "\n",
    "# Volume ë¶„ë¥˜\n",
    "print(\"\\nğŸ“¦ CatBoost - Volume ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "catboost_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=CatBoostClassifier,\n",
    "    param_ranges=catboost_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['catboost']\n",
    ")\n",
    "\n",
    "catboost_search_volume.fit(X_train, y_volume_train, class_weights=volume_class_weights)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {catboost_search_volume.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {catboost_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(catboost_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'CatBoost', 'Volume', catboost_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸŒ² 2. RandomForest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸ”¬ ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ì˜ë¯¸ ìˆëŠ” íŠ¹ì • ê°’ë“¤ë§Œ ì„ ë³„í•˜ì—¬ ì •ë°€í•œ íƒìƒ‰\n",
    "rf_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # ê³„ì‚° ë¹„ìš© ê³ ë ¤í•œ ì„ ë³„ê°’\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],             # ê³¼ì í•© ë°©ì§€ ìµœì ê°’ë“¤\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],             # ì¼ë°˜ì ì¸ ë¶„í•  ê¸°ì¤€ê°’ë“¤\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 12],                # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ê¸°ì¤€\n",
    "    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.5, 0.7], # íŠ¹ì§• ì„ íƒ ì „ëµ\n",
    "    'bootstrap': [True, False],\n",
    "    'max_samples': [0.7, 0.8, 0.9, 1.0]                  # ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œ ë¹„ìœ¨\n",
    "}\n",
    "\n",
    "# Duration ë¶„ë¥˜  \n",
    "print(\"\\nğŸ¯ RandomForest - Duration ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_ranges=rf_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False  # RandomForestëŠ” GPU ë¯¸ì§€ì›\n",
    ")\n",
    "\n",
    "rf_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' ë‚´ì¥\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {rf_search_duration.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {rf_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(rf_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'RandomForest', 'Duration', rf_search_duration.best_params_)\n",
    "\n",
    "# Volume ë¶„ë¥˜\n",
    "print(\"\\nğŸ“¦ RandomForest - Volume ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_ranges=rf_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False\n",
    ")\n",
    "\n",
    "rf_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {rf_search_volume.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {rf_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(rf_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'RandomForest', 'Volume', rf_search_volume.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ 3. LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸ”¬ LightGBM íŠ¹ì„±ì„ ê³ ë ¤í•œ ìµœì í™”ëœ íƒìƒ‰ ê³µê°„\n",
    "lgb_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # ì¡°ê¸° ì¢…ë£Œì™€ í•¨ê»˜ ì‚¬ìš©\n",
    "    'max_depth': [6, 8, 10, 12, 15, 20, -1],             # -1ì€ ì œí•œ ì—†ìŒ\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2], # í•™ìŠµë¥  ì„¸ë°€ ì¡°ì •\n",
    "    'num_leaves': [31, 50, 100, 200, 300, 500],          # íŠ¸ë¦¬ ë³µì¡ë„ ì œì–´\n",
    "    'min_child_samples': [10, 20, 30, 50, 100],          # ê³¼ì í•© ë°©ì§€\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],                   # ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],           # íŠ¹ì§• ìƒ˜í”Œë§\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0, 2.0],               # L1 ì •ê·œí™”\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0]               # L2 ì •ê·œí™”\n",
    "}\n",
    "\n",
    "# Duration ë¶„ë¥˜\n",
    "print(\"\\nğŸ¯ LightGBM - Duration ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgb_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_ranges=lgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['lightgbm']\n",
    ")\n",
    "\n",
    "lgb_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' ë‚´ì¥\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {lgb_search_duration.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {lgb_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(lgb_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'LightGBM', 'Duration', lgb_search_duration.best_params_)\n",
    "\n",
    "# Volume ë¶„ë¥˜\n",
    "print(\"\\nğŸ“¦ LightGBM - Volume ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgb_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_ranges=lgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['lightgbm']\n",
    ")\n",
    "\n",
    "lgb_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {lgb_search_volume.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {lgb_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(lgb_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'LightGBM', 'Volume', lgb_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ 4. XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ) - ğŸ¯ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì™„ë²½ ì§€ì›!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸ”¬ XGBoost ì „ìš© ìµœì í™”: ê° íŒŒë¼ë¯¸í„°ì˜ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•œ ê°’ë“¤\n",
    "xgb_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # early_stoppingê³¼ ì¡°í™”\n",
    "    'max_depth': [3, 6, 8, 10, 12, 15],                  # XGBoost ê¶Œì¥ ê¹Šì´ ë²”ìœ„\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2], # eta ìµœì ê°’ë“¤\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],                   # í–‰ ìƒ˜í”Œë§\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],           # ì—´ ìƒ˜í”Œë§\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 2.0],                   # ìµœì†Œ ë¶„í•  loss\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0, 2.0],               # L1 ì •ê·œí™”\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0],              # L2 ì •ê·œí™”\n",
    "    'min_child_weight': [1, 3, 5, 7, 10]                # ìì‹ ë…¸ë“œ ê°€ì¤‘ì¹˜ í•©\n",
    "}\n",
    "\n",
    "# Duration ë¶„ë¥˜\n",
    "print(\"\\nğŸ¯ XGBoost - Duration ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_ranges=xgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['xgboost']\n",
    ")\n",
    "\n",
    "xgb_search_duration.fit(X_train, y_duration_train, class_weights=duration_class_weights)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {xgb_search_duration.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {xgb_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(xgb_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'XGBoost', 'Duration', xgb_search_duration.best_params_)\n",
    "\n",
    "# Volume ë¶„ë¥˜\n",
    "print(\"\\nğŸ“¦ XGBoost - Volume ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_ranges=xgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['xgboost']\n",
    ")\n",
    "\n",
    "xgb_search_volume.fit(X_train, y_volume_train, class_weights=volume_class_weights)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {xgb_search_volume.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {xgb_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(xgb_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'XGBoost', 'Volume', xgb_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸŒ³ 5. ExtraTrees í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (100íšŒ)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ğŸ”¬ ExtraTrees íŠ¹ì„±: ë¬´ì‘ìœ„ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì•ˆì •ì ì¸ ê°’ë“¤ ìœ„ì£¼ë¡œ íƒìƒ‰\n",
    "et_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # ë†’ì€ ë¬´ì‘ìœ„ì„± ë³´ìƒ\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],             # ê¹Šì´ ì œí•œ ì „ëµ\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],             # ë¶„í•  ì„ê³„ê°’\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 12],                # ë¦¬í”„ í¬ê¸° ì œì–´\n",
    "    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.5, 0.7], # íŠ¹ì§• ì„ íƒ ë‹¤ì–‘ì„±\n",
    "    'bootstrap': [True, False],                           # ìƒ˜í”Œë§ ë°©ì‹\n",
    "    'max_samples': [0.7, 0.8, 0.9, 1.0]                  # ë¶€íŠ¸ìŠ¤íŠ¸ë© í¬ê¸°\n",
    "}\n",
    "\n",
    "# Duration ë¶„ë¥˜\n",
    "print(\"\\nğŸ¯ ExtraTrees - Duration ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=ExtraTreesClassifier,\n",
    "    param_ranges=et_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False  # ExtraTreesëŠ” GPU ë¯¸ì§€ì›\n",
    ")\n",
    "\n",
    "et_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' ë‚´ì¥\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {et_search_duration.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {et_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(et_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'ExtraTrees', 'Duration', et_search_duration.best_params_)\n",
    "\n",
    "# Volume ë¶„ë¥˜\n",
    "print(\"\\nğŸ“¦ ExtraTrees - Volume ë¶„ë¥˜ íŠœë‹...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=ExtraTreesClassifier,\n",
    "    param_ranges=et_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False\n",
    ")\n",
    "\n",
    "et_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"â±ï¸ ì†Œìš”ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ\")\n",
    "print(f\"ğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {et_search_volume.best_params_}\")\n",
    "print(f\"ğŸ¯ êµì°¨ê²€ì¦ ì ìˆ˜: {et_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(et_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'ExtraTrees', 'Volume', et_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ê²°ê³¼ DataFrame ìƒì„±\n",
    "final_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Duration ê²°ê³¼\n",
    "print(\"\\nğŸ¯ Duration ë¶„ë¥˜ ê²°ê³¼:\")\n",
    "duration_results = final_results_df[final_results_df['task'] == 'Duration'].copy()\n",
    "duration_results = duration_results.sort_values('f1_weighted', ascending=False)\n",
    "print(f\"{'ìˆœìœ„':<4} {'ëª¨ë¸':<12} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1(W)':<8} {'F1(M)':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (_, row) in enumerate(duration_results.iterrows(), 1):\n",
    "    print(f\"{i:<4} {row['model']:<12} {row['accuracy']:<8.4f} {row['precision']:<8.4f} \"\n",
    "          f\"{row['recall']:<8.4f} {row['f1_weighted']:<8.4f} {row['f1_macro']:<8.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Volume ë¶„ë¥˜ ê²°ê³¼:\")\n",
    "volume_results = final_results_df[final_results_df['task'] == 'Volume'].copy()\n",
    "volume_results = volume_results.sort_values('f1_weighted', ascending=False)\n",
    "print(f\"{'ìˆœìœ„':<4} {'ëª¨ë¸':<12} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1(W)':<8} {'F1(M)':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (_, row) in enumerate(volume_results.iterrows(), 1):\n",
    "    print(f\"{i:<4} {row['model']:<12} {row['accuracy']:<8.4f} {row['precision']:<8.4f} \"\n",
    "          f\"{row['recall']:<8.4f} {row['f1_weighted']:<8.4f} {row['f1_macro']:<8.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ ì „ì²´ ìµœê³  ì„±ëŠ¥:\")\n",
    "best_duration = duration_results.iloc[0]\n",
    "best_volume = volume_results.iloc[0]\n",
    "print(f\"Duration ìµœê³ : {best_duration['model']} (F1-weighted: {best_duration['f1_weighted']:.4f})\")\n",
    "print(f\"Volume ìµœê³ : {best_volume['model']} (F1-weighted: {best_volume['f1_weighted']:.4f})\")\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!\")\n",
    "print(f\"   - 5ê°œ ëª¨ë¸ Ã— 2ê°œ íƒœìŠ¤í¬ Ã— 100íšŒ = ì´ 1,000íšŒ íŠœë‹\")\n",
    "print(f\"   - GPU ê°€ì† í™œìš©: {sum(gpu_status.values())}ê°œ ëª¨ë¸\")\n",
    "print(f\"   - ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "print(f\"   - {'ğŸ§  Optuna ì§€ëŠ¥í˜• íƒìƒ‰' if OPTUNA_AVAILABLE else 'ğŸ² ëœë¤ íƒìƒ‰'} ì‚¬ìš©\")\n",
    "print(f\"   - ğŸ¯ XGBoost í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì™„ë²½ ì§€ì›!\")\n",
    "print(f\"   - âš–ï¸ Duration+Volume ë³µí•© ì¸µí™” ìƒ˜í”Œë§\")\n",
    "print(f\"   - ğŸ”¬ ì •ë°€í•œ íƒìƒ‰ê³µê°„: suggest_categoricalë¡œ ìµœì í™”\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
