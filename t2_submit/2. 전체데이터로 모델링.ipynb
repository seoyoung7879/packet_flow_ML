{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2464d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 🧹 \"최종 데이터 정제 스크립트\" (이것만 단독으로 실행하세요)\n",
    "# ----------------------------------------------------\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🧹 데이터 정제: Flow-Packet 동기화 중복 제거\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. 원본 데이터 로딩\n",
    "print(\"📂 1단계: 원본 데이터 로딩...\")\n",
    "flow_data = joblib.load(\"task2_data/train_flow_data.pkl\")\n",
    "\n",
    "all_packets = []\n",
    "packet_files = [f\"task2_data/train_packet_data_{i}.pkl\" for i in range(50000, 650000, 50000)]\n",
    "for file_path in packet_files:\n",
    "    if os.path.exists(file_path):\n",
    "        packets_chunk = joblib.load(file_path)\n",
    "        all_packets.extend(packets_chunk)\n",
    "print(f\"✅ Flow/Packet 데이터 로딩 완료\")\n",
    "\n",
    "# 2. 'Flow' 데이터를 기준으로 중복되지 않은 '원본 인덱스' 확보\n",
    "print(\"\\n🔍 2단계: 유효 인덱스 식별...\")\n",
    "initial_rows = len(flow_data)\n",
    "non_duplicate_indices = flow_data.drop_duplicates().index\n",
    "final_rows = len(non_duplicate_indices)\n",
    "print(f\"✅ 유효 인덱스 {final_rows:,}개 확보 (제거될 중복: {initial_rows - final_rows:,}개)\")\n",
    "\n",
    "# 3. '유효 인덱스'를 사용하여 Flow와 Packet 데이터 동시 정제\n",
    "print(\"\\n🔄 3단계: 데이터 동기화 정제...\")\n",
    "# 🔥 오류 수정: reset_index(drop=True)를 절대 사용하지 않아, 원본과의 연결을 유지해야 하지만,\n",
    "# 최종 저장 파일은 0부터 시작하는 인덱스를 갖는 것이 일반적이므로, 여기서는 reset_index를 사용합니다.\n",
    "# 중요한 것은 Packet 데이터도 동일한 순서로 정렬된 후에 저장된다는 점입니다.\n",
    "flow_data_cleaned = flow_data.loc[non_duplicate_indices].reset_index(drop=True)\n",
    "packet_data_cleaned = [all_packets[i] for i in non_duplicate_indices]\n",
    "print(f\"✅ 동기화 완료! 최종 데이터: {len(flow_data_cleaned):,}개\")\n",
    "\n",
    "# 4. 정제된 데이터를 새로운 파일로 저장\n",
    "print(\"\\n💾 4단계: 정제된 데이터셋 저장...\")\n",
    "cleaned_flow_path = \"task2_data/train_flow_data_cleaned.pkl\"\n",
    "cleaned_packet_path = \"task2_data/train_packet_data_cleaned.pkl\"\n",
    "joblib.dump(flow_data_cleaned, cleaned_flow_path)\n",
    "joblib.dump(packet_data_cleaned, cleaned_packet_path)\n",
    "print(f\"✅ 저장 완료: {cleaned_flow_path}, {cleaned_packet_path}\")\n",
    "print(f\"\\n🎉 모든 데이터 정제 작업 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🌍 전체 데이터 균등 층화 샘플링 + 4개 모델 하이퍼파라미터 튜닝\n",
      "================================================================================\n",
      "\n",
      "📂 1단계: 전체 데이터 파일 스캔\n",
      "==================================================\n",
      "✓ 발견: task2_data/train_packet_data_50000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_100000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_150000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_200000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_250000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_300000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_350000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_400000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_450000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_500000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_550000.pkl\n",
      "✓ 발견: task2_data/train_packet_data_600000.pkl\n",
      "\n",
      "📊 총 12개 패킷 파일 발견\n",
      "\n",
      "📈 Flow 데이터 로딩 및 클래스 분포 분석...\n",
      "✓ Flow 데이터 크기: (600000, 11)\n",
      "\n",
      "📊 전체 Duration Class 분포:\n",
      "   Class 0: 23,407개 (3.9%)\n",
      "   Class 1: 212,796개 (35.5%)\n",
      "   Class 2: 140,943개 (23.5%)\n",
      "   Class 3: 222,854개 (37.1%)\n",
      "\n",
      "📊 전체 Volume Class 분포:\n",
      "   Class 0: 205,670개 (34.3%)\n",
      "   Class 1: 145,515개 (24.3%)\n",
      "   Class 2: 213,478개 (35.6%)\n",
      "   Class 3: 35,337개 (5.9%)\n",
      "\n",
      "🎯 2단계: 균등 + 층화 샘플링 전략\n",
      "==================================================\n",
      "🎯 목표 총 샘플 수: 50,000개\n",
      "📁 파일당 균등 샘플 수: 4,166개\n",
      "📊 층화 추출 기준: Duration + Volume 복합 클래스 비율 보존\n",
      "\n",
      "⚖️ 3단계: 각 파일에서 균등 + 층화 샘플링 실행\n",
      "==================================================\n",
      "\n",
      "📁 파일 1/12: task2_data/train_packet_data_50000.pkl\n",
      "   📊 대응 Flow 범위: 0 ~ 49,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(2572), 1: np.int64(20191), 2: np.int64(11916), 3: np.int64(15321)}\n",
      "   📦 Volume 분포: {0: np.int64(18372), 1: np.int64(14692), 2: np.int64(15202), 3: np.int64(1734)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(215), 1: np.int64(1682), 2: np.int64(992), 3: np.int64(1277)}\n",
      "   📦 샘플 Volume: {0: np.int64(1531), 1: np.int64(1224), 2: np.int64(1267), 3: np.int64(144)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.051), 1: np.float64(0.404), 2: np.float64(0.238), 3: np.float64(0.306)} vs 샘플{0: np.float64(0.052), 1: np.float64(0.404), 2: np.float64(0.238), 3: np.float64(0.307)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.367), 1: np.float64(0.294), 2: np.float64(0.304), 3: np.float64(0.035)} vs 샘플{0: np.float64(0.367), 1: np.float64(0.294), 2: np.float64(0.304), 3: np.float64(0.035)}\n",
      "\n",
      "📁 파일 2/12: task2_data/train_packet_data_100000.pkl\n",
      "   📊 대응 Flow 범위: 50,000 ~ 99,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1876), 1: np.int64(16417), 2: np.int64(14377), 3: np.int64(17330)}\n",
      "   📦 Volume 분포: {0: np.int64(15331), 1: np.int64(12220), 2: np.int64(19157), 3: np.int64(3292)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(156), 1: np.int64(1368), 2: np.int64(1197), 3: np.int64(1445)}\n",
      "   📦 샘플 Volume: {0: np.int64(1278), 1: np.int64(1018), 2: np.int64(1596), 3: np.int64(274)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.038), 1: np.float64(0.328), 2: np.float64(0.288), 3: np.float64(0.347)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.328), 2: np.float64(0.287), 3: np.float64(0.347)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)} vs 샘플{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)}\n",
      "\n",
      "📁 파일 3/12: task2_data/train_packet_data_150000.pkl\n",
      "   📊 대응 Flow 범위: 100,000 ~ 149,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1808), 1: np.int64(18190), 2: np.int64(11240), 3: np.int64(18762)}\n",
      "   📦 Volume 분포: {0: np.int64(16568), 1: np.int64(11811), 2: np.int64(18245), 3: np.int64(3376)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(151), 1: np.int64(1515), 2: np.int64(936), 3: np.int64(1564)}\n",
      "   📦 샘플 Volume: {0: np.int64(1381), 1: np.int64(984), 2: np.int64(1520), 3: np.int64(281)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)} vs 샘플{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.068)} vs 샘플{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.067)}\n",
      "\n",
      "📁 파일 4/12: task2_data/train_packet_data_200000.pkl\n",
      "   📊 대응 Flow 범위: 150,000 ~ 199,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1787), 1: np.int64(17473), 2: np.int64(11322), 3: np.int64(19418)}\n",
      "   📦 Volume 분포: {0: np.int64(16828), 1: np.int64(11541), 2: np.int64(18425), 3: np.int64(3206)}\n",
      "   🔗 복합층(D_V): 16개 조합\n",
      "   ❌ 샘플링 실패: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "📁 파일 5/12: task2_data/train_packet_data_250000.pkl\n",
      "   📊 대응 Flow 범위: 200,000 ~ 249,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1829), 1: np.int64(17848), 2: np.int64(11066), 3: np.int64(19257)}\n",
      "   📦 Volume 분포: {0: np.int64(19092), 1: np.int64(11493), 2: np.int64(16530), 3: np.int64(2885)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(156), 1: np.int64(1368), 2: np.int64(1197), 3: np.int64(1445)}\n",
      "   📦 샘플 Volume: {0: np.int64(1278), 1: np.int64(1018), 2: np.int64(1596), 3: np.int64(274)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.038), 1: np.float64(0.328), 2: np.float64(0.288), 3: np.float64(0.347)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.328), 2: np.float64(0.287), 3: np.float64(0.347)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)} vs 샘플{0: np.float64(0.307), 1: np.float64(0.244), 2: np.float64(0.383), 3: np.float64(0.066)}\n",
      "\n",
      "📁 파일 3/12: task2_data/train_packet_data_150000.pkl\n",
      "   📊 대응 Flow 범위: 100,000 ~ 149,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1808), 1: np.int64(18190), 2: np.int64(11240), 3: np.int64(18762)}\n",
      "   📦 Volume 분포: {0: np.int64(16568), 1: np.int64(11811), 2: np.int64(18245), 3: np.int64(3376)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(151), 1: np.int64(1515), 2: np.int64(936), 3: np.int64(1564)}\n",
      "   📦 샘플 Volume: {0: np.int64(1381), 1: np.int64(984), 2: np.int64(1520), 3: np.int64(281)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)} vs 샘플{0: np.float64(0.036), 1: np.float64(0.364), 2: np.float64(0.225), 3: np.float64(0.375)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.068)} vs 샘플{0: np.float64(0.331), 1: np.float64(0.236), 2: np.float64(0.365), 3: np.float64(0.067)}\n",
      "\n",
      "📁 파일 4/12: task2_data/train_packet_data_200000.pkl\n",
      "   📊 대응 Flow 범위: 150,000 ~ 199,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1787), 1: np.int64(17473), 2: np.int64(11322), 3: np.int64(19418)}\n",
      "   📦 Volume 분포: {0: np.int64(16828), 1: np.int64(11541), 2: np.int64(18425), 3: np.int64(3206)}\n",
      "   🔗 복합층(D_V): 16개 조합\n",
      "   ❌ 샘플링 실패: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "📁 파일 5/12: task2_data/train_packet_data_250000.pkl\n",
      "   📊 대응 Flow 범위: 200,000 ~ 249,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1829), 1: np.int64(17848), 2: np.int64(11066), 3: np.int64(19257)}\n",
      "   📦 Volume 분포: {0: np.int64(19092), 1: np.int64(11493), 2: np.int64(16530), 3: np.int64(2885)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(153), 1: np.int64(1487), 2: np.int64(922), 3: np.int64(1604)}\n",
      "   📦 샘플 Volume: {0: np.int64(1591), 1: np.int64(957), 2: np.int64(1378), 3: np.int64(240)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)} vs 샘플{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)}\n",
      "\n",
      "📁 파일 6/12: task2_data/train_packet_data_300000.pkl\n",
      "   📊 대응 Flow 범위: 250,000 ~ 299,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1858), 1: np.int64(17656), 2: np.int64(11544), 3: np.int64(18942)}\n",
      "   📦 Volume 분포: {0: np.int64(17991), 1: np.int64(11939), 2: np.int64(17284), 3: np.int64(2786)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(155), 1: np.int64(1471), 2: np.int64(962), 3: np.int64(1578)}\n",
      "   📦 샘플 Volume: {0: np.int64(1499), 1: np.int64(994), 2: np.int64(1440), 3: np.int64(233)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)} vs 샘플{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)}\n",
      "\n",
      "📁 파일 7/12: task2_data/train_packet_data_350000.pkl\n",
      "   📊 대응 Flow 범위: 300,000 ~ 349,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1811), 1: np.int64(17557), 2: np.int64(10985), 3: np.int64(19647)}\n",
      "   📦 Volume 분포: {0: np.int64(16768), 1: np.int64(11675), 2: np.int64(18284), 3: np.int64(3273)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(151), 1: np.int64(1463), 2: np.int64(915), 3: np.int64(1637)}\n",
      "   📦 샘플 Volume: {0: np.int64(1398), 1: np.int64(973), 2: np.int64(1523), 3: np.int64(272)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)} vs 샘플{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.335), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)} vs 샘플{0: np.float64(0.336), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)}\n",
      "\n",
      "📁 파일 8/12: task2_data/train_packet_data_400000.pkl\n",
      "   📊 대응 Flow 범위: 350,000 ~ 399,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1723), 1: np.int64(17448), 2: np.int64(11855), 3: np.int64(18974)}\n",
      "   📦 Volume 분포: {0: np.int64(16599), 1: np.int64(12429), 2: np.int64(17997), 3: np.int64(2975)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(144), 1: np.int64(1454), 2: np.int64(987), 3: np.int64(1581)}\n",
      "   📦 샘플 Volume: {0: np.int64(1382), 1: np.int64(1036), 2: np.int64(1500), 3: np.int64(248)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.034), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.379)} vs 샘플{0: np.float64(0.035), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.38)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)} vs 샘플{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)}\n",
      "\n",
      "📁 파일 9/12: task2_data/train_packet_data_450000.pkl\n",
      "   📊 대응 Flow 범위: 400,000 ~ 449,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1916), 1: np.int64(17477), 2: np.int64(11474), 3: np.int64(19133)}\n",
      "   📦 Volume 분포: {0: np.int64(16755), 1: np.int64(11853), 2: np.int64(18227), 3: np.int64(3165)}\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(153), 1: np.int64(1487), 2: np.int64(922), 3: np.int64(1604)}\n",
      "   📦 샘플 Volume: {0: np.int64(1591), 1: np.int64(957), 2: np.int64(1378), 3: np.int64(240)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.357), 2: np.float64(0.221), 3: np.float64(0.385)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)} vs 샘플{0: np.float64(0.382), 1: np.float64(0.23), 2: np.float64(0.331), 3: np.float64(0.058)}\n",
      "\n",
      "📁 파일 6/12: task2_data/train_packet_data_300000.pkl\n",
      "   📊 대응 Flow 범위: 250,000 ~ 299,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1858), 1: np.int64(17656), 2: np.int64(11544), 3: np.int64(18942)}\n",
      "   📦 Volume 분포: {0: np.int64(17991), 1: np.int64(11939), 2: np.int64(17284), 3: np.int64(2786)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(155), 1: np.int64(1471), 2: np.int64(962), 3: np.int64(1578)}\n",
      "   📦 샘플 Volume: {0: np.int64(1499), 1: np.int64(994), 2: np.int64(1440), 3: np.int64(233)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)} vs 샘플{0: np.float64(0.037), 1: np.float64(0.353), 2: np.float64(0.231), 3: np.float64(0.379)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)} vs 샘플{0: np.float64(0.36), 1: np.float64(0.239), 2: np.float64(0.346), 3: np.float64(0.056)}\n",
      "\n",
      "📁 파일 7/12: task2_data/train_packet_data_350000.pkl\n",
      "   📊 대응 Flow 범위: 300,000 ~ 349,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1811), 1: np.int64(17557), 2: np.int64(10985), 3: np.int64(19647)}\n",
      "   📦 Volume 분포: {0: np.int64(16768), 1: np.int64(11675), 2: np.int64(18284), 3: np.int64(3273)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(151), 1: np.int64(1463), 2: np.int64(915), 3: np.int64(1637)}\n",
      "   📦 샘플 Volume: {0: np.int64(1398), 1: np.int64(973), 2: np.int64(1523), 3: np.int64(272)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)} vs 샘플{0: np.float64(0.036), 1: np.float64(0.351), 2: np.float64(0.22), 3: np.float64(0.393)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.335), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)} vs 샘플{0: np.float64(0.336), 1: np.float64(0.234), 2: np.float64(0.366), 3: np.float64(0.065)}\n",
      "\n",
      "📁 파일 8/12: task2_data/train_packet_data_400000.pkl\n",
      "   📊 대응 Flow 범위: 350,000 ~ 399,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1723), 1: np.int64(17448), 2: np.int64(11855), 3: np.int64(18974)}\n",
      "   📦 Volume 분포: {0: np.int64(16599), 1: np.int64(12429), 2: np.int64(17997), 3: np.int64(2975)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(144), 1: np.int64(1454), 2: np.int64(987), 3: np.int64(1581)}\n",
      "   📦 샘플 Volume: {0: np.int64(1382), 1: np.int64(1036), 2: np.int64(1500), 3: np.int64(248)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.034), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.379)} vs 샘플{0: np.float64(0.035), 1: np.float64(0.349), 2: np.float64(0.237), 3: np.float64(0.38)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)} vs 샘플{0: np.float64(0.332), 1: np.float64(0.249), 2: np.float64(0.36), 3: np.float64(0.06)}\n",
      "\n",
      "📁 파일 9/12: task2_data/train_packet_data_450000.pkl\n",
      "   📊 대응 Flow 범위: 400,000 ~ 449,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1916), 1: np.int64(17477), 2: np.int64(11474), 3: np.int64(19133)}\n",
      "   📦 Volume 분포: {0: np.int64(16755), 1: np.int64(11853), 2: np.int64(18227), 3: np.int64(3165)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(160), 1: np.int64(1456), 2: np.int64(955), 3: np.int64(1595)}\n",
      "   📦 샘플 Volume: {0: np.int64(1396), 1: np.int64(988), 2: np.int64(1518), 3: np.int64(264)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.038), 1: np.float64(0.35), 2: np.float64(0.229), 3: np.float64(0.383)} vs 샘플{0: np.float64(0.038), 1: np.float64(0.349), 2: np.float64(0.229), 3: np.float64(0.383)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.365), 3: np.float64(0.063)} vs 샘플{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.364), 3: np.float64(0.063)}\n",
      "\n",
      "📁 파일 10/12: task2_data/train_packet_data_500000.pkl\n",
      "   📊 대응 Flow 범위: 450,000 ~ 499,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1764), 1: np.int64(16808), 2: np.int64(12126), 3: np.int64(19302)}\n",
      "   📦 Volume 분포: {0: np.int64(16306), 1: np.int64(11789), 2: np.int64(18675), 3: np.int64(3230)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(147), 1: np.int64(1400), 2: np.int64(1010), 3: np.int64(1609)}\n",
      "   📦 샘플 Volume: {0: np.int64(1358), 1: np.int64(982), 2: np.int64(1557), 3: np.int64(269)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.243), 3: np.float64(0.386)} vs 샘플{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.242), 3: np.float64(0.386)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)} vs 샘플{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)}\n",
      "\n",
      "📁 파일 11/12: task2_data/train_packet_data_550000.pkl\n",
      "   📊 대응 Flow 범위: 500,000 ~ 549,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1954), 1: np.int64(17090), 2: np.int64(11755), 3: np.int64(19201)}\n",
      "   📦 Volume 분포: {0: np.int64(16045), 1: np.int64(11920), 2: np.int64(18991), 3: np.int64(3044)}\n",
      "   🔗 복합층(D_V): 16개 조합\n",
      "   ❌ 샘플링 실패: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "📁 파일 12/12: task2_data/train_packet_data_600000.pkl\n",
      "   📊 대응 Flow 범위: 550,000 ~ 599,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(2509), 1: np.int64(18641), 2: np.int64(11283), 3: np.int64(17567)}\n",
      "   📦 Volume 분포: {0: np.int64(19015), 1: np.int64(12153), 2: np.int64(16461), 3: np.int64(2371)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(160), 1: np.int64(1456), 2: np.int64(955), 3: np.int64(1595)}\n",
      "   📦 샘플 Volume: {0: np.int64(1396), 1: np.int64(988), 2: np.int64(1518), 3: np.int64(264)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.038), 1: np.float64(0.35), 2: np.float64(0.229), 3: np.float64(0.383)} vs 샘플{0: np.float64(0.038), 1: np.float64(0.349), 2: np.float64(0.229), 3: np.float64(0.383)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.365), 3: np.float64(0.063)} vs 샘플{0: np.float64(0.335), 1: np.float64(0.237), 2: np.float64(0.364), 3: np.float64(0.063)}\n",
      "\n",
      "📁 파일 10/12: task2_data/train_packet_data_500000.pkl\n",
      "   📊 대응 Flow 범위: 450,000 ~ 499,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1764), 1: np.int64(16808), 2: np.int64(12126), 3: np.int64(19302)}\n",
      "   📦 Volume 분포: {0: np.int64(16306), 1: np.int64(11789), 2: np.int64(18675), 3: np.int64(3230)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(147), 1: np.int64(1400), 2: np.int64(1010), 3: np.int64(1609)}\n",
      "   📦 샘플 Volume: {0: np.int64(1358), 1: np.int64(982), 2: np.int64(1557), 3: np.int64(269)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.243), 3: np.float64(0.386)} vs 샘플{0: np.float64(0.035), 1: np.float64(0.336), 2: np.float64(0.242), 3: np.float64(0.386)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)} vs 샘플{0: np.float64(0.326), 1: np.float64(0.236), 2: np.float64(0.374), 3: np.float64(0.065)}\n",
      "\n",
      "📁 파일 11/12: task2_data/train_packet_data_550000.pkl\n",
      "   📊 대응 Flow 범위: 500,000 ~ 549,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(1954), 1: np.int64(17090), 2: np.int64(11755), 3: np.int64(19201)}\n",
      "   📦 Volume 분포: {0: np.int64(16045), 1: np.int64(11920), 2: np.int64(18991), 3: np.int64(3044)}\n",
      "   🔗 복합층(D_V): 16개 조합\n",
      "   ❌ 샘플링 실패: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "\n",
      "📁 파일 12/12: task2_data/train_packet_data_600000.pkl\n",
      "   📊 대응 Flow 범위: 550,000 ~ 599,999 (50,000개)\n",
      "   🎯 Duration 분포: {0: np.int64(2509), 1: np.int64(18641), 2: np.int64(11283), 3: np.int64(17567)}\n",
      "   📦 Volume 분포: {0: np.int64(19015), 1: np.int64(12153), 2: np.int64(16461), 3: np.int64(2371)}\n",
      "   🔗 복합층(D_V): 15개 조합\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(209), 1: np.int64(1553), 2: np.int64(940), 3: np.int64(1464)}\n",
      "   📦 샘플 Volume: {0: np.int64(1584), 1: np.int64(1013), 2: np.int64(1371), 3: np.int64(198)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)} vs 샘플{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.047)} vs 샘플{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.048)}\n",
      "\n",
      "✅ 전체 샘플링 완료!\n",
      "📊 총 샘플링된 인덱스: 41,660개\n",
      "🎯 목표 대비 달성률: 83.3%\n",
      "\n",
      "📊 4단계: 최종 샘플링 결과 검증\n",
      "==================================================\n",
      "🎯 최종 샘플링된 Duration Class 분포:\n",
      "   Class 0: 1,641개 (3.9%) [원본: 3.9%]\n",
      "   Class 1: 14,849개 (35.6%) [원본: 35.5%]\n",
      "   Class 2: 9,816개 (23.6%) [원본: 23.5%]\n",
      "   Class 3: 15,354개 (36.9%) [원본: 37.1%]\n",
      "\n",
      "📦 최종 샘플링된 Volume Class 분포:\n",
      "   Class 0: 14,398개 (34.6%) [원본: 34.3%]\n",
      "   Class 1: 10,169개 (24.4%) [원본: 24.3%]\n",
      "   Class 2: 14,670개 (35.2%) [원본: 35.6%]\n",
      "   Class 3: 2,423개 (5.8%) [원본: 5.9%]\n",
      "\n",
      "⚖️ 복합 층화 비율 보존도:\n",
      "   Duration: 99.4%\n",
      "   Volume: 99.1%\n",
      "   전체 평균: 99.2% (100%에 가까울수록 완벽)\n",
      "\n",
      "✅ 복합 층화 샘플링 완료! Duration+Volume 동시 비율 보존으로 최고 품질 샘플 확보! 🎉\n",
      "   ✅ 샘플링 완료: 4,166개\n",
      "   📈 샘플 Duration: {0: np.int64(209), 1: np.int64(1553), 2: np.int64(940), 3: np.int64(1464)}\n",
      "   📦 샘플 Volume: {0: np.int64(1584), 1: np.int64(1013), 2: np.int64(1371), 3: np.int64(198)}\n",
      "   ⚖️ Duration 비율보존: 원본{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)} vs 샘플{0: np.float64(0.05), 1: np.float64(0.373), 2: np.float64(0.226), 3: np.float64(0.351)}\n",
      "   ⚖️ Volume 비율보존: 원본{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.047)} vs 샘플{0: np.float64(0.38), 1: np.float64(0.243), 2: np.float64(0.329), 3: np.float64(0.048)}\n",
      "\n",
      "✅ 전체 샘플링 완료!\n",
      "📊 총 샘플링된 인덱스: 41,660개\n",
      "🎯 목표 대비 달성률: 83.3%\n",
      "\n",
      "📊 4단계: 최종 샘플링 결과 검증\n",
      "==================================================\n",
      "🎯 최종 샘플링된 Duration Class 분포:\n",
      "   Class 0: 1,641개 (3.9%) [원본: 3.9%]\n",
      "   Class 1: 14,849개 (35.6%) [원본: 35.5%]\n",
      "   Class 2: 9,816개 (23.6%) [원본: 23.5%]\n",
      "   Class 3: 15,354개 (36.9%) [원본: 37.1%]\n",
      "\n",
      "📦 최종 샘플링된 Volume Class 분포:\n",
      "   Class 0: 14,398개 (34.6%) [원본: 34.3%]\n",
      "   Class 1: 10,169개 (24.4%) [원본: 24.3%]\n",
      "   Class 2: 14,670개 (35.2%) [원본: 35.6%]\n",
      "   Class 3: 2,423개 (5.8%) [원본: 5.9%]\n",
      "\n",
      "⚖️ 복합 층화 비율 보존도:\n",
      "   Duration: 99.4%\n",
      "   Volume: 99.1%\n",
      "   전체 평균: 99.2% (100%에 가까울수록 완벽)\n",
      "\n",
      "✅ 복합 층화 샘플링 완료! Duration+Volume 동시 비율 보존으로 최고 품질 샘플 확보! 🎉\n"
     ]
    }
   ],
   "source": [
    "# 🌍 전체 데이터 균등 층화 샘플링 및 모델링\n",
    "print(\"=\"*80)\n",
    "print(\"🌍 전체 데이터 균등 층화 샘플링 + 4개 모델 하이퍼파라미터 튜닝\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 1단계: 전체 파일 목록 및 정보 확인\n",
    "print(\"\\n📂 1단계: 전체 데이터 파일 스캔\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 패킷 데이터 파일들 (50000부터 600000까지 50000씩)\n",
    "packet_files = []\n",
    "for i in range(50000, 650000, 50000):  # 50000, 100000, ..., 600000\n",
    "    file_path = f\"task2_data/train_packet_data_{i}.pkl\"\n",
    "    if os.path.exists(file_path):\n",
    "        packet_files.append(file_path)\n",
    "        print(f\"✓ 발견: {file_path}\")\n",
    "    else:\n",
    "        print(f\"❌ 없음: {file_path}\")\n",
    "\n",
    "print(f\"\\n📊 총 {len(packet_files)}개 패킷 파일 발견\")\n",
    "\n",
    "# Flow 데이터 로딩 (클래스 분포 확인용)\n",
    "print(f\"\\n📈 Flow 데이터 로딩 및 클래스 분포 분석...\")\n",
    "flow_data = joblib.load(\"task2_data/train_flow_data.pkl\")\n",
    "print(f\"✓ Flow 데이터 크기: {flow_data.shape}\")\n",
    "\n",
    "# 전체 클래스 분포 확인\n",
    "duration_dist = flow_data['duration_class'].value_counts().sort_index()\n",
    "volume_dist = flow_data['volume_class'].value_counts().sort_index()\n",
    "\n",
    "print(f\"\\n📊 전체 Duration Class 분포:\")\n",
    "for cls, count in duration_dist.items():\n",
    "    percentage = (count / len(flow_data)) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}개 ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 전체 Volume Class 분포:\")\n",
    "for cls, count in volume_dist.items():\n",
    "    percentage = (count / len(flow_data)) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}개 ({percentage:.1f}%)\")\n",
    "\n",
    "# 2단계: 균등 + 층화 샘플링 전략\n",
    "print(f\"\\n🎯 2단계: 균등 + 층화 샘플링 전략\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 목표 샘플 수 설정\n",
    "target_total_samples = 50000  # 전체 목표 샘플 수\n",
    "samples_per_file = target_total_samples // len(packet_files)  # 파일당 균등 샘플 수\n",
    "\n",
    "print(f\"🎯 목표 총 샘플 수: {target_total_samples:,}개\")\n",
    "print(f\"📁 파일당 균등 샘플 수: {samples_per_file:,}개\")\n",
    "print(f\"📊 층화 추출 기준: Duration + Volume 복합 클래스 비율 보존\")\n",
    "\n",
    "def stratified_sample_from_file(file_path, sample_size, flow_data_subset):\n",
    "    \"\"\"\n",
    "    파일 내에서 층화 추출 수행 - Duration + Volume 복합 층화\n",
    "    \"\"\"\n",
    "    # Duration + Volume 복합 클래스 기준으로 층화 추출\n",
    "    stratify_key = flow_data_subset['duration_class'].astype(str) + '_' + flow_data_subset['volume_class'].astype(str)\n",
    "    \n",
    "    stratified_sampler = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        train_size=sample_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    indices = np.arange(len(flow_data_subset))\n",
    "    stratified_indices, _ = next(stratified_sampler.split(indices, stratify_key))\n",
    "    \n",
    "    return stratified_indices\n",
    "\n",
    "# 3단계: 각 파일에서 균등 + 층화 샘플링\n",
    "print(f\"\\n⚖️ 3단계: 각 파일에서 균등 + 층화 샘플링 실행\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sampled_indices = []\n",
    "total_sampled = 0\n",
    "\n",
    "for i, file_path in enumerate(packet_files, 1):\n",
    "    print(f\"\\n📁 파일 {i}/{len(packet_files)}: {file_path}\")\n",
    "    \n",
    "    # 파일 번호 추출 (예: train_packet_data_100000.pkl -> 100000)\n",
    "    file_num = int(file_path.split('_')[-1].split('.')[0])\n",
    "    \n",
    "    # 해당 파일에 대응하는 flow 데이터 범위\n",
    "    start_idx = (i-1) * 50000\n",
    "    end_idx = min(start_idx + 50000, len(flow_data))\n",
    "    flow_subset = flow_data.iloc[start_idx:end_idx].copy()\n",
    "    flow_subset.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"   📊 대응 Flow 범위: {start_idx:,} ~ {end_idx-1:,} ({len(flow_subset):,}개)\")\n",
    "    \n",
    "    # 이 구간의 클래스 분포\n",
    "    subset_duration_dist = flow_subset['duration_class'].value_counts().sort_index()\n",
    "    subset_volume_dist = flow_subset['volume_class'].value_counts().sort_index()\n",
    "    print(f\"   🎯 Duration 분포: {dict(subset_duration_dist)}\")\n",
    "    print(f\"   📦 Volume 분포: {dict(subset_volume_dist)}\")\n",
    "    \n",
    "    # 복합 층화 키 분포 확인\n",
    "    subset_stratify_key = flow_subset['duration_class'].astype(str) + '_' + flow_subset['volume_class'].astype(str)\n",
    "    subset_combined_dist = subset_stratify_key.value_counts().sort_index()\n",
    "    print(f\"   🔗 복합층(D_V): {len(subset_combined_dist)}개 조합\")\n",
    "    \n",
    "    # 층화 샘플링 실행\n",
    "    try:\n",
    "        actual_sample_size = min(samples_per_file, len(flow_subset))\n",
    "        stratified_indices = stratified_sample_from_file(file_path, actual_sample_size, flow_subset)\n",
    "        \n",
    "        # 전체 인덱스로 변환 (start_idx 더하기)\n",
    "        global_indices = [start_idx + idx for idx in stratified_indices]\n",
    "        sampled_indices.extend(global_indices)\n",
    "        \n",
    "        total_sampled += len(stratified_indices)\n",
    "        \n",
    "        # 샘플링된 데이터의 클래스 분포 확인\n",
    "        sampled_flow = flow_subset.iloc[stratified_indices]\n",
    "        sampled_duration_dist = sampled_flow['duration_class'].value_counts().sort_index()\n",
    "        sampled_volume_dist = sampled_flow['volume_class'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"   ✅ 샘플링 완료: {len(stratified_indices):,}개\")\n",
    "        print(f\"   📈 샘플 Duration: {dict(sampled_duration_dist)}\")\n",
    "        print(f\"   📦 샘플 Volume: {dict(sampled_volume_dist)}\")\n",
    "        \n",
    "        # 복합 비율 보존 확인\n",
    "        original_duration_ratios = (subset_duration_dist / len(flow_subset)).round(3)\n",
    "        sampled_duration_ratios = (sampled_duration_dist / len(stratified_indices)).round(3)\n",
    "        original_volume_ratios = (subset_volume_dist / len(flow_subset)).round(3)\n",
    "        sampled_volume_ratios = (sampled_volume_dist / len(stratified_indices)).round(3)\n",
    "        \n",
    "        print(f\"   ⚖️ Duration 비율보존: 원본{dict(original_duration_ratios)} vs 샘플{dict(sampled_duration_ratios)}\")\n",
    "        print(f\"   ⚖️ Volume 비율보존: 원본{dict(original_volume_ratios)} vs 샘플{dict(sampled_volume_ratios)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 샘플링 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ 전체 샘플링 완료!\")\n",
    "print(f\"📊 총 샘플링된 인덱스: {len(sampled_indices):,}개\")\n",
    "print(f\"🎯 목표 대비 달성률: {len(sampled_indices)/target_total_samples*100:.1f}%\")\n",
    "\n",
    "# 4단계: 샘플링된 데이터의 최종 클래스 분포 확인\n",
    "print(f\"\\n📊 4단계: 최종 샘플링 결과 검증\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 샘플링된 flow 데이터\n",
    "sampled_flow_data = flow_data.iloc[sampled_indices].copy()\n",
    "sampled_flow_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_duration_dist = sampled_flow_data['duration_class'].value_counts().sort_index()\n",
    "final_volume_dist = sampled_flow_data['volume_class'].value_counts().sort_index()\n",
    "\n",
    "print(f\"🎯 최종 샘플링된 Duration Class 분포:\")\n",
    "for cls, count in final_duration_dist.items():\n",
    "    original_ratio = duration_dist[cls] / len(flow_data) * 100\n",
    "    sampled_ratio = count / len(sampled_flow_data) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}개 ({sampled_ratio:.1f}%) [원본: {original_ratio:.1f}%]\")\n",
    "\n",
    "print(f\"\\n📦 최종 샘플링된 Volume Class 분포:\")\n",
    "for cls, count in final_volume_dist.items():\n",
    "    original_ratio = volume_dist[cls] / len(flow_data) * 100\n",
    "    sampled_ratio = count / len(sampled_flow_data) * 100\n",
    "    print(f\"   Class {int(cls)}: {count:,}개 ({sampled_ratio:.1f}%) [원본: {original_ratio:.1f}%]\")\n",
    "\n",
    "# 클래스 비율 보존도 측정 (Duration + Volume 모두)\n",
    "duration_preservation = []\n",
    "for cls in final_duration_dist.index:\n",
    "    original_ratio = duration_dist[cls] / len(flow_data)\n",
    "    sampled_ratio = final_duration_dist[cls] / len(sampled_flow_data)\n",
    "    preservation = min(sampled_ratio/original_ratio, original_ratio/sampled_ratio) * 100\n",
    "    duration_preservation.append(preservation)\n",
    "\n",
    "volume_preservation = []\n",
    "for cls in final_volume_dist.index:\n",
    "    original_ratio = volume_dist[cls] / len(flow_data)\n",
    "    sampled_ratio = final_volume_dist[cls] / len(sampled_flow_data)\n",
    "    preservation = min(sampled_ratio/original_ratio, original_ratio/sampled_ratio) * 100\n",
    "    volume_preservation.append(preservation)\n",
    "\n",
    "avg_duration_preservation = np.mean(duration_preservation)\n",
    "avg_volume_preservation = np.mean(volume_preservation)\n",
    "overall_preservation = (avg_duration_preservation + avg_volume_preservation) / 2\n",
    "\n",
    "print(f\"\\n⚖️ 복합 층화 비율 보존도:\")\n",
    "print(f\"   Duration: {avg_duration_preservation:.1f}%\")\n",
    "print(f\"   Volume: {avg_volume_preservation:.1f}%\") \n",
    "print(f\"   전체 평균: {overall_preservation:.1f}% (100%에 가까울수록 완벽)\")\n",
    "\n",
    "print(f\"\\n✅ 복합 층화 샘플링 완료! Duration+Volume 동시 비율 보존으로 최고 품질 샘플 확보! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f367f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 5단계: 고급 특징 엔지니어링\n",
      "==================================================\n",
      "🚀 고급 특징 추출 시작...\n",
      "   📂 로딩: task2_data/train_packet_data_50000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_100000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_100000.pkl\n",
      "      진행률: 5,000개 완료\n",
      "      진행률: 5,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_150000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_150000.pkl\n",
      "      진행률: 10,000개 완료\n",
      "      진행률: 10,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_250000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_250000.pkl\n",
      "      진행률: 15,000개 완료\n",
      "      진행률: 15,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_300000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_300000.pkl\n",
      "      진행률: 20,000개 완료\n",
      "      진행률: 20,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_350000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_350000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_400000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_400000.pkl\n",
      "      진행률: 25,000개 완료\n",
      "      진행률: 25,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_450000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_450000.pkl\n",
      "      진행률: 30,000개 완료\n",
      "      진행률: 30,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_500000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_500000.pkl\n",
      "      진행률: 35,000개 완료\n",
      "      진행률: 35,000개 완료\n",
      "   📂 로딩: task2_data/train_packet_data_600000.pkl\n",
      "   📂 로딩: task2_data/train_packet_data_600000.pkl\n",
      "      진행률: 40,000개 완료\n",
      "      진행률: 40,000개 완료\n",
      "\n",
      "✅ 고급 특징 추출 완료!\n",
      "📊 추출된 특징 수: 41,660개\n",
      "🎯 유효 인덱스 수: 41,660개\n",
      "\n",
      "✅ 고급 특징 추출 완료!\n",
      "📊 추출된 특징 수: 41,660개\n",
      "🎯 유효 인덱스 수: 41,660개\n",
      "✓ 최종 특징 행렬 크기: (41660, 44)\n",
      "✓ 타겟 데이터 크기: (41660, 11)\n",
      "\n",
      "🌟 생성된 고급 특징들 (18개):\n",
      "    1. 🧠 ip_len_mean_13\n",
      "    2. 🧠 ip_len_std_13\n",
      "    3. 🧠 ip_len_max_13\n",
      "    4. 🧠 ip_len_min_13\n",
      "    5. 🧠 ip_len_range_13\n",
      "    6. 🧠 ip_len_median_13\n",
      "    7. 🧠 ip_len_trend\n",
      "    8. 🧠 ip_len_volatility\n",
      "    9. 🧠 inter_time_mean_13\n",
      "   10. 🧠 inter_time_std_13\n",
      "   ... 외 8개 더\n",
      "\n",
      "✅ 5단계 완료: 고급 특징 엔지니어링 완료! 🎉\n",
      "✓ 최종 특징 행렬 크기: (41660, 44)\n",
      "✓ 타겟 데이터 크기: (41660, 11)\n",
      "\n",
      "🌟 생성된 고급 특징들 (18개):\n",
      "    1. 🧠 ip_len_mean_13\n",
      "    2. 🧠 ip_len_std_13\n",
      "    3. 🧠 ip_len_max_13\n",
      "    4. 🧠 ip_len_min_13\n",
      "    5. 🧠 ip_len_range_13\n",
      "    6. 🧠 ip_len_median_13\n",
      "    7. 🧠 ip_len_trend\n",
      "    8. 🧠 ip_len_volatility\n",
      "    9. 🧠 inter_time_mean_13\n",
      "   10. 🧠 inter_time_std_13\n",
      "   ... 외 8개 더\n",
      "\n",
      "✅ 5단계 완료: 고급 특징 엔지니어링 완료! 🎉\n"
     ]
    }
   ],
   "source": [
    "# 5단계: 고급 특징 엔지니어링 (최대 3개 패킷 활용)\n",
    "print(f\"\\n🔧 5단계: 고급 특징 엔지니어링\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def extract_advanced_features(sampled_indices, packet_files):\n",
    "    \"\"\"\n",
    "    샘플링된 인덱스를 기반으로 고급 특징 추출\n",
    "    \"\"\"\n",
    "    print(\"🚀 고급 특징 추출 시작...\")\n",
    "    \n",
    "    features_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for global_idx in sampled_indices:\n",
    "        try:\n",
    "            # 어떤 파일에서 가져올지 결정\n",
    "            file_idx = global_idx // 50000\n",
    "            local_idx = global_idx % 50000\n",
    "            \n",
    "            if file_idx >= len(packet_files):\n",
    "                continue\n",
    "                \n",
    "            # 해당 파일 로딩 (캐시 활용)\n",
    "            file_path = packet_files[file_idx]\n",
    "            \n",
    "            if not hasattr(extract_advanced_features, 'cache'):\n",
    "                extract_advanced_features.cache = {}\n",
    "            \n",
    "            if file_path not in extract_advanced_features.cache:\n",
    "                print(f\"   📂 로딩: {file_path}\")\n",
    "                extract_advanced_features.cache[file_path] = joblib.load(file_path)\n",
    "            \n",
    "            packet_data = extract_advanced_features.cache[file_path]\n",
    "            \n",
    "            # 로컬 인덱스가 범위를 벗어나면 스킵\n",
    "            if local_idx >= len(packet_data):\n",
    "                continue\n",
    "                \n",
    "            packet_df = packet_data[local_idx]\n",
    "            valid_packets = packet_df.dropna()\n",
    "            \n",
    "            if len(valid_packets) >= 1:  # 최소 1개 패킷 필요\n",
    "                features = extract_packet_features(valid_packets)\n",
    "                features_list.append(features)\n",
    "                valid_indices.append(global_idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "        if len(features_list) % 5000 == 0:\n",
    "            print(f\"      진행률: {len(features_list):,}개 완료\")\n",
    "    \n",
    "    return features_list, valid_indices\n",
    "\n",
    "def extract_packet_features(packets):\n",
    "    \"\"\"\n",
    "    패킷들로부터 고급 특징 추출 (최대 3개 패킷만 사용)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    num_packets = min(3, len(packets))\n",
    "    packets = packets.iloc[:num_packets]\n",
    "    \n",
    "    # 숫자형 컬럼 확인\n",
    "    numeric_cols = packets.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # === 기본 특징들 ===\n",
    "    for col in numeric_cols:\n",
    "        features[f'first_{col}'] = packets.iloc[0][col] if col in packets.columns else 0\n",
    "        features[f'second_{col}'] = packets.iloc[1][col] if col in packets.columns and len(packets) > 1 else 0\n",
    "    \n",
    "    # === 🌟 통계 특징 (1~3개 패킷) ===\n",
    "    if 'ip_len' in packets.columns:\n",
    "        ip_lens = packets['ip_len'].values\n",
    "        features['ip_len_mean_13'] = np.mean(ip_lens)\n",
    "        features['ip_len_std_13'] = np.std(ip_lens) if len(ip_lens) > 1 else 0\n",
    "        features['ip_len_max_13'] = np.max(ip_lens)\n",
    "        features['ip_len_min_13'] = np.min(ip_lens)\n",
    "        features['ip_len_range_13'] = np.max(ip_lens) - np.min(ip_lens)\n",
    "        features['ip_len_median_13'] = np.median(ip_lens)\n",
    "        \n",
    "        # 변화 패턴\n",
    "        if len(ip_lens) >= 3:\n",
    "            diffs = np.diff(ip_lens)\n",
    "            features['ip_len_trend'] = 1 if np.mean(diffs) > 0 else (-1 if np.mean(diffs) < 0 else 0)\n",
    "            features['ip_len_volatility'] = np.std(diffs) if len(diffs) > 1 else 0\n",
    "        else:\n",
    "            features['ip_len_trend'] = 0\n",
    "            features['ip_len_volatility'] = 0\n",
    "    \n",
    "    # 패킷 간 시간 통계\n",
    "    if 'packet_capture_time' in packets.columns:\n",
    "        try:\n",
    "            times = pd.to_datetime(packets['packet_capture_time'])\n",
    "            time_diffs = np.diff(times).astype('timedelta64[us]').astype(float)\n",
    "            \n",
    "            if len(time_diffs) > 0:\n",
    "                features['inter_time_mean_13'] = np.mean(time_diffs)\n",
    "                features['inter_time_std_13'] = np.std(time_diffs) if len(time_diffs) > 1 else 0\n",
    "                features['inter_time_max_13'] = np.max(time_diffs)\n",
    "                features['inter_time_min_13'] = np.min(time_diffs)\n",
    "                features['timing_consistency'] = np.std(time_diffs) / (np.mean(time_diffs) + 1)\n",
    "            else:\n",
    "                for key in ['inter_time_mean_13', 'inter_time_std_13', 'inter_time_max_13', \n",
    "                           'inter_time_min_13', 'timing_consistency']:\n",
    "                    features[key] = 0\n",
    "        except:\n",
    "            for key in ['inter_time_mean_13', 'inter_time_std_13', 'inter_time_max_13', \n",
    "                       'inter_time_min_13', 'timing_consistency']:\n",
    "                features[key] = 0\n",
    "    \n",
    "    # TCP 효율성\n",
    "    if 'tcp_len' in packets.columns and 'ip_len' in packets.columns:\n",
    "        tcp_lens = packets['tcp_len'].values\n",
    "        features['tcp_len_mean_13'] = np.mean(tcp_lens)\n",
    "        features['tcp_len_std_13'] = np.std(tcp_lens) if len(tcp_lens) > 1 else 0\n",
    "        features['tcp_len_sum_13'] = np.sum(tcp_lens)\n",
    "        \n",
    "        total_ip = np.sum(packets['ip_len'])\n",
    "        total_tcp = np.sum(tcp_lens)\n",
    "        features['tcp_efficiency_13'] = total_tcp / max(total_ip, 1)\n",
    "    \n",
    "    # === 🌟 TCP 플래그 패턴 특징 ===\n",
    "    if 'tcp_flags' in packets.columns:\n",
    "        flags = packets['tcp_flags'].values\n",
    "        \n",
    "        # 개별 플래그\n",
    "        features['has_syn'] = int(any(flag & 0x02 for flag in flags))\n",
    "        features['has_ack'] = int(any(flag & 0x10 for flag in flags))\n",
    "        features['has_fin'] = int(any(flag & 0x01 for flag in flags))\n",
    "        features['has_rst'] = int(any(flag & 0x04 for flag in flags))\n",
    "        features['has_psh'] = int(any(flag & 0x08 for flag in flags))\n",
    "        \n",
    "        # 핸드셰이크 완전성\n",
    "        if len(flags) >= 3:\n",
    "            first_syn = (flags[0] & 0x02) != 0\n",
    "            second_syn_ack = (flags[1] & 0x12) == 0x12\n",
    "            third_ack = (flags[2] & 0x10) != 0\n",
    "            features['is_handshake_complete'] = int(first_syn and second_syn_ack and third_ack)\n",
    "            \n",
    "            has_fin_ack = any((flag & 0x11) == 0x11 for flag in flags)\n",
    "            features['is_graceful_close'] = int(has_fin_ack)\n",
    "        else:\n",
    "            features['is_handshake_complete'] = 0\n",
    "            features['is_graceful_close'] = 0\n",
    "        \n",
    "        features['flag_diversity'] = len(set(flags))\n",
    "        psh_count = sum(1 for flag in flags if flag & 0x08)\n",
    "        features['push_frequency'] = psh_count / len(flags)\n",
    "    \n",
    "    # === 🌟 추가 고급 특징들 ===\n",
    "    # 크기 변화 패턴\n",
    "    if 'ip_len' in packets.columns and len(packets) >= 3:\n",
    "        sizes = packets['ip_len'].values\n",
    "        increases = sum(1 for i in range(1, len(sizes)) if sizes[i] > sizes[i-1])\n",
    "        decreases = sum(1 for i in range(1, len(sizes)) if sizes[i] < sizes[i-1])\n",
    "        \n",
    "        features['size_increase_count'] = increases\n",
    "        features['size_decrease_count'] = decreases\n",
    "        features['size_stability'] = sum(1 for i in range(1, len(sizes)) if sizes[i] == sizes[i-1])\n",
    "    \n",
    "    # 비율 특징\n",
    "    if 'tcp_len' in packets.columns and 'ip_len' in packets.columns:\n",
    "        tcp_to_ip_ratio_1 = packets.iloc[0]['tcp_len'] / max(packets.iloc[0]['ip_len'], 1)\n",
    "        features['tcp_to_ip_ratio_first'] = tcp_to_ip_ratio_1\n",
    "        \n",
    "        if len(packets) > 1:\n",
    "            tcp_to_ip_ratio_2 = packets.iloc[1]['tcp_len'] / max(packets.iloc[1]['ip_len'], 1)\n",
    "            features['tcp_to_ip_ratio_second'] = tcp_to_ip_ratio_2\n",
    "    \n",
    "    # 기존 호환성 특징\n",
    "    features['inter_packet_time_us'] = features.get('inter_time_mean_13', 0)\n",
    "    features['ip_len_diff'] = features.get('second_ip_len', 0) - features.get('first_ip_len', 0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 고급 특징 추출 실행\n",
    "advanced_features_list, valid_global_indices = extract_advanced_features(sampled_indices, packet_files)\n",
    "\n",
    "print(f\"\\n✅ 고급 특징 추출 완료!\")\n",
    "print(f\"📊 추출된 특징 수: {len(advanced_features_list):,}개\")\n",
    "print(f\"🎯 유효 인덱스 수: {len(valid_global_indices):,}개\")\n",
    "\n",
    "# 데이터프레임 생성\n",
    "advanced_features_df = pd.DataFrame(advanced_features_list)\n",
    "advanced_features_df = advanced_features_df.fillna(0)\n",
    "\n",
    "# 대응하는 타겟 데이터\n",
    "valid_flow_data = flow_data.iloc[valid_global_indices].copy()\n",
    "valid_flow_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"✓ 최종 특징 행렬 크기: {advanced_features_df.shape}\")\n",
    "print(f\"✓ 타겟 데이터 크기: {valid_flow_data.shape}\")\n",
    "\n",
    "# 새로운 고급 특징들 확인\n",
    "new_advanced_features = [col for col in advanced_features_df.columns \n",
    "                        if any(pattern in col for pattern in ['_13', 'handshake', 'efficiency', \n",
    "                                                             'consistency', 'diversity', 'frequency',\n",
    "                                                             'stability', 'trend', 'volatility'])]\n",
    "\n",
    "print(f\"\\n🌟 생성된 고급 특징들 ({len(new_advanced_features)}개):\")\n",
    "for i, feat in enumerate(new_advanced_features[:10], 1):  # 처음 10개만 표시\n",
    "    print(f\"   {i:2d}. 🧠 {feat}\")\n",
    "if len(new_advanced_features) > 10:\n",
    "    print(f\"   ... 외 {len(new_advanced_features)-10}개 더\")\n",
    "\n",
    "print(f\"\\n✅ 5단계 완료: 고급 특징 엔지니어링 완료! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 6단계: 5개 모델 100회 하이퍼파라미터 튜닝\n",
      "================================================================================\n",
      "✅ Optuna를 사용한 지능형 하이퍼파라미터 탐색\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-07 23:52:59,064] A new study created in memory with name: no-name-793dd8bb-e312-4f41-afcd-3ec8e2894613\n",
      "[W 2025-08-07 23:52:59,067] Trial 0 failed with parameters: {'iterations': 300, 'depth': 10, 'learning_rate': 0.03, 'l2_leaf_reg': 10, 'border_count': 254, 'bagging_temperature': 0.5, 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'} because of the following error: TypeError(\"got an unexpected keyword argument 'fit_params'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hg226\\AppData\\Local\\Temp\\ipykernel_5696\\2007194405.py\", line 185, in objective\n",
      "    scores = cross_val_score(\n",
      "        model, self.X, self.y, cv=self.cv, scoring=self.scoring,\n",
      "        fit_params=self.fit_params if hasattr(self, 'fit_params') else None,\n",
      "        n_jobs=1\n",
      "    )\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 194, in wrapper\n",
      "    params = func_sig.bind(*args, **kwargs)\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3264, in bind\n",
      "    return self._bind(args, kwargs)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3253, in _bind\n",
      "    raise TypeError(\n",
      "        'got an unexpected keyword argument {arg!r}'.format(\n",
      "            arg=next(iter(kwargs))))\n",
      "TypeError: got an unexpected keyword argument 'fit_params'\n",
      "[W 2025-08-07 23:52:59,115] Trial 0 failed with value None.\n",
      "[W 2025-08-07 23:52:59,067] Trial 0 failed with parameters: {'iterations': 300, 'depth': 10, 'learning_rate': 0.03, 'l2_leaf_reg': 10, 'border_count': 254, 'bagging_temperature': 0.5, 'bootstrap_type': 'Bernoulli', 'leaf_estimation_method': 'Gradient'} because of the following error: TypeError(\"got an unexpected keyword argument 'fit_params'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hg226\\AppData\\Local\\Temp\\ipykernel_5696\\2007194405.py\", line 185, in objective\n",
      "    scores = cross_val_score(\n",
      "        model, self.X, self.y, cv=self.cv, scoring=self.scoring,\n",
      "        fit_params=self.fit_params if hasattr(self, 'fit_params') else None,\n",
      "        n_jobs=1\n",
      "    )\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 194, in wrapper\n",
      "    params = func_sig.bind(*args, **kwargs)\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3264, in bind\n",
      "    return self._bind(args, kwargs)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py\", line 3253, in _bind\n",
      "    raise TypeError(\n",
      "        'got an unexpected keyword argument {arg!r}'.format(\n",
      "            arg=next(iter(kwargs))))\n",
      "TypeError: got an unexpected keyword argument 'fit_params'\n",
      "[W 2025-08-07 23:52:59,115] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ GPU 상태:\n",
      "   NVIDIA GPU: ✅\n",
      "   CatBoost GPU: ✅\n",
      "   XGBoost GPU: ✅\n",
      "   LightGBM GPU: ✅\n",
      "\n",
      "📊 데이터 준비 완료:\n",
      "   학습 데이터: 33,328개\n",
      "   검증 데이터: 8,332개\n",
      "   특징 수: 44개\n",
      "\n",
      "⚖️ 클래스 가중치:\n",
      "   Duration: {0: np.float64(6.345773038842346), 1: np.float64(0.701405842242613), 2: np.float64(1.0609957977842863), 3: np.float64(0.678335911422291)}\n",
      "   Volume: {0: np.float64(0.723389477339816), 1: np.float64(1.0242163491087892), 2: np.float64(0.7099522835719154), 3: np.float64(4.29706034038164)}\n",
      "\n",
      "================================================================================\n",
      "🐱 1. CatBoost 하이퍼파라미터 튜닝 (100회)\n",
      "================================================================================\n",
      "\n",
      "🎯 CatBoost - Duration 분류 튜닝...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 364\u001b[39m\n\u001b[32m    352\u001b[39m start_time = time.time()\n\u001b[32m    354\u001b[39m catboost_search_duration = OptunaHyperparameterSearch(\n\u001b[32m    355\u001b[39m     model_class=CatBoostClassifier,\n\u001b[32m    356\u001b[39m     param_ranges=catboost_param_ranges,\n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m     gpu_available=gpu_status[\u001b[33m'\u001b[39m\u001b[33mcatboost\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    362\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43mcatboost_search_duration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_duration_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration_class_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⏱️ 소요시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m초\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    366\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🏆 최적 파라미터: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatboost_search_duration.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 220\u001b[39m, in \u001b[36mOptunaHyperparameterSearch.fit\u001b[39m\u001b[34m(self, X, y, class_weights)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   진행률: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | 최고: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m study = optuna.create_study(\n\u001b[32m    216\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    217\u001b[39m     sampler=optuna.samplers.TPESampler(seed=\u001b[38;5;28mself\u001b[39m.random_state)\n\u001b[32m    218\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28mself\u001b[39m.best_params_ = study.best_params\n\u001b[32m    228\u001b[39m \u001b[38;5;28mself\u001b[39m.best_score_ = study.best_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 185\u001b[39m, in \u001b[36mOptunaHyperparameterSearch.objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# 교차 검증\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfit_params\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3264\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3261\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3262\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hg226\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3253\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3243\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3244\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3245\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3250\u001b[39m             ),\n\u001b[32m   3251\u001b[39m         )\n\u001b[32m   3252\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3254\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3255\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'fit_params'"
     ]
    }
   ],
   "source": [
    "# 6단계: 5개 모델 100회 하이퍼파라미터 튜닝 (GPU 가속 + 클래스 가중치 + Optuna)\n",
    "print(f\"\\n🚀 6단계: 5개 모델 100회 하이퍼파라미터 튜닝\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import subprocess\n",
    "\n",
    "# Optuna 설치 및 임포트\n",
    "try:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)  # 진행상황 표시\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"✅ Optuna를 사용한 지능형 하이퍼파라미터 탐색\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"❌ Optuna 없음 - 기존 랜덤 서치 사용\")\n",
    "    print(\"   설치: pip install optuna\")\n",
    "\n",
    "# GPU 확인 함수 (강화 버전)\n",
    "def check_gpu_advanced():\n",
    "    gpu_status = {}\n",
    "    \n",
    "    # NVIDIA GPU 확인\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
    "        gpu_status['nvidia'] = result.returncode == 0\n",
    "    except:\n",
    "        gpu_status['nvidia'] = False\n",
    "    \n",
    "    # CatBoost GPU 테스트\n",
    "    try:\n",
    "        test_cat = CatBoostClassifier(task_type='GPU', iterations=1, verbose=False)\n",
    "        test_cat.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['catboost'] = True\n",
    "    except:\n",
    "        gpu_status['catboost'] = False\n",
    "    \n",
    "    # XGBoost GPU 테스트\n",
    "    try:\n",
    "        test_xgb = xgb.XGBClassifier(tree_method='gpu_hist', n_estimators=1)\n",
    "        test_xgb.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['xgboost'] = True\n",
    "    except:\n",
    "        gpu_status['xgboost'] = False\n",
    "    \n",
    "    # LightGBM GPU 테스트\n",
    "    try:\n",
    "        test_lgb = lgb.LGBMClassifier(device='gpu', n_estimators=1, verbose=-1)\n",
    "        test_lgb.fit([[1, 2], [3, 4]], [0, 1])\n",
    "        gpu_status['lightgbm'] = True\n",
    "    except:\n",
    "        gpu_status['lightgbm'] = False\n",
    "    \n",
    "    return gpu_status\n",
    "\n",
    "gpu_status = check_gpu_advanced()\n",
    "print(f\"🖥️ GPU 상태:\")\n",
    "print(f\"   NVIDIA GPU: {'✅' if gpu_status['nvidia'] else '❌'}\")\n",
    "print(f\"   CatBoost GPU: {'✅' if gpu_status['catboost'] else '❌'}\")\n",
    "print(f\"   XGBoost GPU: {'✅' if gpu_status['xgboost'] else '❌'}\")\n",
    "print(f\"   LightGBM GPU: {'✅' if gpu_status['lightgbm'] else '❌'}\")\n",
    "\n",
    "# 데이터 준비\n",
    "X = advanced_features_df\n",
    "y_duration = valid_flow_data['duration_class']\n",
    "y_volume = valid_flow_data['volume_class']\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_duration_train, y_duration_test = train_test_split(\n",
    "    X, y_duration, test_size=0.2, random_state=42, stratify=y_duration\n",
    ")\n",
    "_, _, y_volume_train, y_volume_test = train_test_split(\n",
    "    X, y_volume, test_size=0.2, random_state=42, stratify=y_volume\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 데이터 준비 완료:\")\n",
    "print(f\"   학습 데이터: {X_train.shape[0]:,}개\")\n",
    "print(f\"   검증 데이터: {X_test.shape[0]:,}개\") \n",
    "print(f\"   특징 수: {X_train.shape[1]:,}개\")\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "duration_classes = np.unique(y_duration_train)\n",
    "duration_weights = compute_class_weight('balanced', classes=duration_classes, y=y_duration_train)\n",
    "duration_class_weights = {int(cls): weight for cls, weight in zip(duration_classes, duration_weights)}\n",
    "\n",
    "volume_classes = np.unique(y_volume_train)\n",
    "volume_weights = compute_class_weight('balanced', classes=volume_classes, y=y_volume_train)\n",
    "volume_class_weights = {int(cls): weight for cls, weight in zip(volume_classes, volume_weights)}\n",
    "\n",
    "print(f\"\\n⚖️ 클래스 가중치:\")\n",
    "print(f\"   Duration: {duration_class_weights}\")\n",
    "print(f\"   Volume: {volume_class_weights}\")\n",
    "\n",
    "# 결과 저장\n",
    "results = {\n",
    "    'model': [], 'task': [], 'accuracy': [], 'precision': [], \n",
    "    'recall': [], 'f1_weighted': [], 'f1_macro': [], 'best_params': []\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, task_name, best_params):\n",
    "    \"\"\"모델 평가\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    results['model'].append(model_name)\n",
    "    results['task'].append(task_name)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1_weighted'].append(f1_weighted)\n",
    "    results['f1_macro'].append(f1_macro)\n",
    "    results['best_params'].append(str(best_params)[:100])  # 길이 제한\n",
    "    \n",
    "    print(f\"   정확도: {accuracy:.4f} | 정밀도: {precision:.4f} | 재현율: {recall:.4f}\")\n",
    "    print(f\"   F1(weighted): {f1_weighted:.4f} | F1(macro): {f1_macro:.4f}\")\n",
    "\n",
    "# 클래스 가중치를 샘플 가중치로 변환하는 함수\n",
    "def create_sample_weights(y, class_weights):\n",
    "    \"\"\"클래스 가중치를 샘플별 가중치 배열로 변환\"\"\"\n",
    "    return np.array([class_weights[cls] for cls in y])\n",
    "\n",
    "# Optuna 기반 지능형 하이퍼파라미터 튜닝 클래스\n",
    "class OptunaHyperparameterSearch:\n",
    "    def __init__(self, model_class, param_ranges, n_trials, cv, scoring, random_state, gpu_available=False):\n",
    "        self.model_class = model_class\n",
    "        self.param_ranges = param_ranges\n",
    "        self.n_trials = n_trials\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.random_state = random_state\n",
    "        self.gpu_available = gpu_available\n",
    "        self.best_estimator_ = None\n",
    "        self.best_params_ = None\n",
    "        self.best_score_ = None\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        # 파라미터 샘플링 (Optuna 권장 방식: 이산값은 categorical로 처리)\n",
    "        params = {}\n",
    "        for param_name, param_config in self.param_ranges.items():\n",
    "            if isinstance(param_config, list):\n",
    "                # 🔬 프로 팁: 모든 리스트는 categorical로 처리하여 정확한 탐색 공간 제어\n",
    "                params[param_name] = trial.suggest_categorical(param_name, param_config)\n",
    "        \n",
    "        # 모델별 특수 설정\n",
    "        model_params = {}\n",
    "        if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "            model_params.update({\n",
    "                'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                'random_seed': self.random_state,\n",
    "                'verbose': False,\n",
    "                'eval_metric': 'MultiClass'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'XGBClassifier':\n",
    "            model_params.update({\n",
    "                'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                'eval_metric': 'mlogloss'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "            model_params.update({\n",
    "                'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                'random_state': self.random_state,\n",
    "                'verbose': -1,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1\n",
    "            })\n",
    "        elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "            model_params.update({\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1\n",
    "            })\n",
    "        \n",
    "        # 모델 생성\n",
    "        model = self.model_class(**{**model_params, **params})\n",
    "        \n",
    "        # 교차 검증 - XGBoost sample_weight 문제 해결\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        from sklearn.metrics import f1_score\n",
    "        import numpy as np\n",
    "        \n",
    "        # 수동 교차 검증으로 sample_weight 문제 해결\n",
    "        skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(self.X, self.y):\n",
    "            X_train_fold, X_val_fold = self.X.iloc[train_idx], self.X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = self.y.iloc[train_idx], self.y.iloc[val_idx]\n",
    "            \n",
    "            # 모델 학습\n",
    "            if hasattr(self, 'fit_params') and self.fit_params:\n",
    "                # XGBoost의 경우 sample_weight 적용\n",
    "                if 'sample_weight' in self.fit_params:\n",
    "                    fold_sample_weights = self.fit_params['sample_weight'][train_idx]\n",
    "                    model.fit(X_train_fold, y_train_fold, sample_weight=fold_sample_weights)\n",
    "                else:\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "            else:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # 예측 및 점수 계산\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            score = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def fit(self, X, y, class_weights=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # 클래스 가중치 처리\n",
    "        self.fit_params = {}\n",
    "        if class_weights is not None:\n",
    "            if self.model_class.__name__ == 'XGBClassifier':\n",
    "                # XGBoost는 sample_weight 사용\n",
    "                sample_weights = create_sample_weights(y, class_weights)\n",
    "                self.fit_params = {'sample_weight': sample_weights}\n",
    "            elif self.model_class.__name__ == 'CatBoostClassifier':\n",
    "                # CatBoost는 class_weights 파라미터 사용\n",
    "                self.class_weights = class_weights\n",
    "            else:\n",
    "                # 다른 모델들은 class_weight='balanced' 사용\n",
    "                pass\n",
    "        \n",
    "        if OPTUNA_AVAILABLE:\n",
    "            # Optuna 사용 - 진행상황 콜백 추가\n",
    "            def progress_callback(study, trial):\n",
    "                print(f\"   진행률: ({trial.number + 1:3d}/{self.n_trials}) F1: {trial.value:.4f} | 최고: {study.best_value:.4f}\")\n",
    "            \n",
    "            study = optuna.create_study(\n",
    "                direction='maximize',\n",
    "                sampler=optuna.samplers.TPESampler(seed=self.random_state)\n",
    "            )\n",
    "            \n",
    "            study.optimize(\n",
    "                self.objective, \n",
    "                n_trials=self.n_trials, \n",
    "                callbacks=[progress_callback],\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            \n",
    "            self.best_params_ = study.best_params\n",
    "            self.best_score_ = study.best_value\n",
    "        else:\n",
    "            # 기존 랜덤 서치 사용 - 진행상황 표시 추가\n",
    "            from sklearn.model_selection import ParameterSampler\n",
    "            param_list = list(ParameterSampler(self.param_ranges, n_iter=self.n_trials, random_state=self.random_state))\n",
    "            \n",
    "            best_score = -np.inf\n",
    "            best_params = None\n",
    "            \n",
    "            for i, params in enumerate(param_list, 1):\n",
    "                print(f\"   진행률: ({i:3d}/{self.n_trials}) 테스트 중...\", end=' ')\n",
    "                \n",
    "                try:\n",
    "                    model_params = {}\n",
    "                    if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "                        model_params.update({\n",
    "                            'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                            'random_seed': self.random_state,\n",
    "                            'verbose': False,\n",
    "                            'eval_metric': 'MultiClass'\n",
    "                        })\n",
    "                    elif self.model_class.__name__ == 'XGBClassifier':\n",
    "                        model_params.update({\n",
    "                            'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                            'random_state': self.random_state,\n",
    "                            'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                            'eval_metric': 'mlogloss'\n",
    "                        })\n",
    "                    elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "                        model_params.update({\n",
    "                            'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                            'random_state': self.random_state,\n",
    "                            'verbose': -1,\n",
    "                            'n_jobs': -1 if not self.gpu_available else 1\n",
    "                        })\n",
    "                    elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "                        model_params.update({\n",
    "                            'random_state': self.random_state,\n",
    "                            'n_jobs': -1\n",
    "                        })\n",
    "                    \n",
    "                    model = self.model_class(**{**model_params, **params})\n",
    "                    \n",
    "                    # 수동 교차 검증으로 sample_weight 문제 해결\n",
    "                    from sklearn.model_selection import StratifiedKFold\n",
    "                    from sklearn.metrics import f1_score\n",
    "                    import numpy as np\n",
    "                    \n",
    "                    skf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state)\n",
    "                    scores = []\n",
    "                    \n",
    "                    for train_idx, val_idx in skf.split(X, y):\n",
    "                        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                        \n",
    "                        # 각 모델별 복사본 생성\n",
    "                        fold_model = self.model_class(**{**model_params, **params})\n",
    "                        \n",
    "                        # 모델 학습\n",
    "                        if hasattr(self, 'fit_params') and self.fit_params:\n",
    "                            # XGBoost의 경우 sample_weight 적용\n",
    "                            if 'sample_weight' in self.fit_params:\n",
    "                                fold_sample_weights = self.fit_params['sample_weight'][train_idx]\n",
    "                                fold_model.fit(X_train_fold, y_train_fold, sample_weight=fold_sample_weights)\n",
    "                            else:\n",
    "                                fold_model.fit(X_train_fold, y_train_fold)\n",
    "                        else:\n",
    "                            fold_model.fit(X_train_fold, y_train_fold)\n",
    "                        \n",
    "                        # 예측 및 점수 계산\n",
    "                        y_pred_fold = fold_model.predict(X_val_fold)\n",
    "                        score = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "                        scores.append(score)\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    print(f\"F1: {avg_score:.4f} | 최고: {best_score:.4f}\")\n",
    "                    \n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = params\n",
    "                except Exception as e:\n",
    "                    print(f\"실패\")\n",
    "                    continue\n",
    "            \n",
    "            self.best_params_ = best_params\n",
    "            self.best_score_ = best_score\n",
    "        \n",
    "        # 최적 모델 생성 및 학습\n",
    "        model_params = {}\n",
    "        if self.model_class.__name__ == 'CatBoostClassifier':\n",
    "            model_params.update({\n",
    "                'task_type': 'GPU' if self.gpu_available else 'CPU',\n",
    "                'class_weights': class_weights if class_weights else None,\n",
    "                'random_seed': self.random_state,\n",
    "                'verbose': False,\n",
    "                'eval_metric': 'MultiClass'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'XGBClassifier':\n",
    "            model_params.update({\n",
    "                'tree_method': 'gpu_hist' if self.gpu_available else 'hist',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1,\n",
    "                'eval_metric': 'mlogloss'\n",
    "            })\n",
    "        elif self.model_class.__name__ == 'LGBMClassifier':\n",
    "            model_params.update({\n",
    "                'device': 'gpu' if self.gpu_available else 'cpu',\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                'verbose': -1,\n",
    "                'n_jobs': -1 if not self.gpu_available else 1\n",
    "            })\n",
    "        elif self.model_class.__name__ in ['RandomForestClassifier', 'ExtraTreesClassifier']:\n",
    "            model_params.update({\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': -1\n",
    "            })\n",
    "        \n",
    "        self.best_estimator_ = self.model_class(**{**model_params, **self.best_params_})\n",
    "        \n",
    "        # XGBoost의 경우 sample_weight와 함께 학습\n",
    "        if self.model_class.__name__ == 'XGBClassifier' and class_weights:\n",
    "            sample_weights = create_sample_weights(y, class_weights)\n",
    "            self.best_estimator_.fit(X, y, sample_weight=sample_weights)\n",
    "        else:\n",
    "            self.best_estimator_.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🐱 1. CatBoost 하이퍼파라미터 튜닝 (100회)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🔬 프로 팁: 이산값들은 정확히 지정된 값들만 탐색하도록 설계\n",
    "catboost_param_ranges = {\n",
    "    'iterations': [200, 300, 400, 500, 600],           # 정확히 이 값들만 탐색\n",
    "    'depth': [6, 8, 10, 12, 14],                       # 7, 9, 11, 13은 제외\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15],  # 실험에서 검증된 값들만\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 10, 15],\n",
    "    'border_count': [64, 128, 254],                    # CatBoost 권장값들만\n",
    "    'bagging_temperature': [0.5, 1.0, 1.5, 2.0],\n",
    "    'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],\n",
    "    'leaf_estimation_method': ['Newton', 'Gradient']\n",
    "}\n",
    "\n",
    "# Duration 분류\n",
    "print(\"\\n🎯 CatBoost - Duration 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "catboost_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=CatBoostClassifier,\n",
    "    param_ranges=catboost_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['catboost']\n",
    ")\n",
    "\n",
    "catboost_search_duration.fit(X_train, y_duration_train, class_weights=duration_class_weights)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {catboost_search_duration.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {catboost_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(catboost_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'CatBoost', 'Duration', catboost_search_duration.best_params_)\n",
    "\n",
    "# Volume 분류\n",
    "print(\"\\n📦 CatBoost - Volume 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "catboost_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=CatBoostClassifier,\n",
    "    param_ranges=catboost_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['catboost']\n",
    ")\n",
    "\n",
    "catboost_search_volume.fit(X_train, y_volume_train, class_weights=volume_class_weights)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {catboost_search_volume.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {catboost_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(catboost_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'CatBoost', 'Volume', catboost_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🌲 2. RandomForest 하이퍼파라미터 튜닝 (100회)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🔬 각 파라미터마다 의미 있는 특정 값들만 선별하여 정밀한 탐색\n",
    "rf_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # 계산 비용 고려한 선별값\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],             # 과적합 방지 최적값들\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],             # 일반적인 분할 기준값들\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 12],                # 리프 노드 최소 샘플 기준\n",
    "    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.5, 0.7], # 특징 선택 전략\n",
    "    'bootstrap': [True, False],\n",
    "    'max_samples': [0.7, 0.8, 0.9, 1.0]                  # 부트스트랩 샘플 비율\n",
    "}\n",
    "\n",
    "# Duration 분류  \n",
    "print(\"\\n🎯 RandomForest - Duration 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_ranges=rf_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False  # RandomForest는 GPU 미지원\n",
    ")\n",
    "\n",
    "rf_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' 내장\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {rf_search_duration.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {rf_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(rf_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'RandomForest', 'Duration', rf_search_duration.best_params_)\n",
    "\n",
    "# Volume 분류\n",
    "print(\"\\n📦 RandomForest - Volume 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=RandomForestClassifier,\n",
    "    param_ranges=rf_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False\n",
    ")\n",
    "\n",
    "rf_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {rf_search_volume.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {rf_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(rf_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'RandomForest', 'Volume', rf_search_volume.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 3. LightGBM 하이퍼파라미터 튜닝 (100회)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🔬 LightGBM 특성을 고려한 최적화된 탐색 공간\n",
    "lgb_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # 조기 종료와 함께 사용\n",
    "    'max_depth': [6, 8, 10, 12, 15, 20, -1],             # -1은 제한 없음\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2], # 학습률 세밀 조정\n",
    "    'num_leaves': [31, 50, 100, 200, 300, 500],          # 트리 복잡도 제어\n",
    "    'min_child_samples': [10, 20, 30, 50, 100],          # 과적합 방지\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],                   # 샘플링 비율\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],           # 특징 샘플링\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0, 2.0],               # L1 정규화\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0]               # L2 정규화\n",
    "}\n",
    "\n",
    "# Duration 분류\n",
    "print(\"\\n🎯 LightGBM - Duration 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgb_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_ranges=lgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['lightgbm']\n",
    ")\n",
    "\n",
    "lgb_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' 내장\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {lgb_search_duration.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {lgb_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(lgb_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'LightGBM', 'Duration', lgb_search_duration.best_params_)\n",
    "\n",
    "# Volume 분류\n",
    "print(\"\\n📦 LightGBM - Volume 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgb_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=lgb.LGBMClassifier,\n",
    "    param_ranges=lgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['lightgbm']\n",
    ")\n",
    "\n",
    "lgb_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {lgb_search_volume.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {lgb_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(lgb_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'LightGBM', 'Volume', lgb_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 4. XGBoost 하이퍼파라미터 튜닝 (100회) - 🎯 클래스 가중치 완벽 지원!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🔬 XGBoost 전용 최적화: 각 파라미터의 상호작용을 고려한 값들\n",
    "xgb_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # early_stopping과 조화\n",
    "    'max_depth': [3, 6, 8, 10, 12, 15],                  # XGBoost 권장 깊이 범위\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2], # eta 최적값들\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],                   # 행 샘플링\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],           # 열 샘플링\n",
    "    'gamma': [0, 0.1, 0.5, 1.0, 2.0],                   # 최소 분할 loss\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1.0, 2.0],               # L1 정규화\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0],              # L2 정규화\n",
    "    'min_child_weight': [1, 3, 5, 7, 10]                # 자식 노드 가중치 합\n",
    "}\n",
    "\n",
    "# Duration 분류\n",
    "print(\"\\n🎯 XGBoost - Duration 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_ranges=xgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['xgboost']\n",
    ")\n",
    "\n",
    "xgb_search_duration.fit(X_train, y_duration_train, class_weights=duration_class_weights)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {xgb_search_duration.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {xgb_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(xgb_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'XGBoost', 'Duration', xgb_search_duration.best_params_)\n",
    "\n",
    "# Volume 분류\n",
    "print(\"\\n📦 XGBoost - Volume 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=xgb.XGBClassifier,\n",
    "    param_ranges=xgb_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=gpu_status['xgboost']\n",
    ")\n",
    "\n",
    "xgb_search_volume.fit(X_train, y_volume_train, class_weights=volume_class_weights)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {xgb_search_volume.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {xgb_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(xgb_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'XGBoost', 'Volume', xgb_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🌳 5. ExtraTrees 하이퍼파라미터 튜닝 (100회)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 🔬 ExtraTrees 특성: 무작위성이 높으므로 안정적인 값들 위주로 탐색\n",
    "et_param_ranges = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],     # 높은 무작위성 보상\n",
    "    'max_depth': [10, 15, 20, 25, 30, None],             # 깊이 제한 전략\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],             # 분할 임계값\n",
    "    'min_samples_leaf': [1, 2, 4, 8, 12],                # 리프 크기 제어\n",
    "    'max_features': ['sqrt', 'log2', 0.2, 0.3, 0.5, 0.7], # 특징 선택 다양성\n",
    "    'bootstrap': [True, False],                           # 샘플링 방식\n",
    "    'max_samples': [0.7, 0.8, 0.9, 1.0]                  # 부트스트랩 크기\n",
    "}\n",
    "\n",
    "# Duration 분류\n",
    "print(\"\\n🎯 ExtraTrees - Duration 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_search_duration = OptunaHyperparameterSearch(\n",
    "    model_class=ExtraTreesClassifier,\n",
    "    param_ranges=et_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False  # ExtraTrees는 GPU 미지원\n",
    ")\n",
    "\n",
    "et_search_duration.fit(X_train, y_duration_train, class_weights=None)  # class_weight='balanced' 내장\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {et_search_duration.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {et_search_duration.best_score_:.4f}\")\n",
    "evaluate_model(et_search_duration.best_estimator_, X_test, y_duration_test, \n",
    "               'ExtraTrees', 'Duration', et_search_duration.best_params_)\n",
    "\n",
    "# Volume 분류\n",
    "print(\"\\n📦 ExtraTrees - Volume 분류 튜닝...\")\n",
    "start_time = time.time()\n",
    "\n",
    "et_search_volume = OptunaHyperparameterSearch(\n",
    "    model_class=ExtraTreesClassifier,\n",
    "    param_ranges=et_param_ranges,\n",
    "    n_trials=100,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted',\n",
    "    random_state=42,\n",
    "    gpu_available=False\n",
    ")\n",
    "\n",
    "et_search_volume.fit(X_train, y_volume_train, class_weights=None)\n",
    "print(f\"⏱️ 소요시간: {time.time() - start_time:.1f}초\")\n",
    "print(f\"🏆 최적 파라미터: {et_search_volume.best_params_}\")\n",
    "print(f\"🎯 교차검증 점수: {et_search_volume.best_score_:.4f}\")\n",
    "evaluate_model(et_search_volume.best_estimator_, X_test, y_volume_test, \n",
    "               'ExtraTrees', 'Volume', et_search_volume.best_params_)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏆 최종 성능 비교 결과\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "final_results_df = pd.DataFrame(results)\n",
    "\n",
    "# Duration 결과\n",
    "print(\"\\n🎯 Duration 분류 결과:\")\n",
    "duration_results = final_results_df[final_results_df['task'] == 'Duration'].copy()\n",
    "duration_results = duration_results.sort_values('f1_weighted', ascending=False)\n",
    "print(f\"{'순위':<4} {'모델':<12} {'정확도':<8} {'정밀도':<8} {'재현율':<8} {'F1(W)':<8} {'F1(M)':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (_, row) in enumerate(duration_results.iterrows(), 1):\n",
    "    print(f\"{i:<4} {row['model']:<12} {row['accuracy']:<8.4f} {row['precision']:<8.4f} \"\n",
    "          f\"{row['recall']:<8.4f} {row['f1_weighted']:<8.4f} {row['f1_macro']:<8.4f}\")\n",
    "\n",
    "print(\"\\n📦 Volume 분류 결과:\")\n",
    "volume_results = final_results_df[final_results_df['task'] == 'Volume'].copy()\n",
    "volume_results = volume_results.sort_values('f1_weighted', ascending=False)\n",
    "print(f\"{'순위':<4} {'모델':<12} {'정확도':<8} {'정밀도':<8} {'재현율':<8} {'F1(W)':<8} {'F1(M)':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for i, (_, row) in enumerate(volume_results.iterrows(), 1):\n",
    "    print(f\"{i:<4} {row['model']:<12} {row['accuracy']:<8.4f} {row['precision']:<8.4f} \"\n",
    "          f\"{row['recall']:<8.4f} {row['f1_weighted']:<8.4f} {row['f1_macro']:<8.4f}\")\n",
    "\n",
    "print(\"\\n📈 전체 최고 성능:\")\n",
    "best_duration = duration_results.iloc[0]\n",
    "best_volume = volume_results.iloc[0]\n",
    "print(f\"Duration 최고: {best_duration['model']} (F1-weighted: {best_duration['f1_weighted']:.4f})\")\n",
    "print(f\"Volume 최고: {best_volume['model']} (F1-weighted: {best_volume['f1_weighted']:.4f})\")\n",
    "\n",
    "print(f\"\\n✅ 전체 하이퍼파라미터 튜닝 완료!\")\n",
    "print(f\"   - 5개 모델 × 2개 태스크 × 100회 = 총 1,000회 튜닝\")\n",
    "print(f\"   - GPU 가속 활용: {sum(gpu_status.values())}개 모델\")\n",
    "print(f\"   - 고급 특징 엔지니어링 + 클래스 가중치 적용\")\n",
    "print(f\"   - {'🧠 Optuna 지능형 탐색' if OPTUNA_AVAILABLE else '🎲 랜덤 탐색'} 사용\")\n",
    "print(f\"   - 🎯 XGBoost 클래스 가중치 완벽 지원!\")\n",
    "print(f\"   - ⚖️ Duration+Volume 복합 층화 샘플링\")\n",
    "print(f\"   - 🔬 정밀한 탐색공간: suggest_categorical로 최적화\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
